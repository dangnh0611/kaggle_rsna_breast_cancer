{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T18:33:10.172647Z",
     "iopub.status.busy": "2023-02-12T18:33:10.172193Z",
     "iopub.status.idle": "2023-02-12T18:33:13.821095Z",
     "shell.execute_reply": "2023-02-12T18:33:13.819877Z",
     "shell.execute_reply.started": "2023-02-12T18:33:10.172608Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# #https://www.kaggle.com/code/tivfrvqhs5/torch-tensorrt-infer-fp16-and-fp32-benchmarks/notebook\n",
    "\n",
    "# import os\n",
    "# os.environ['CUDA_MODULE_LOADING']='LAZY'\n",
    "\n",
    "# try: \n",
    "#     import torch_tensorrt\n",
    "    \n",
    "# except:\n",
    "#     #upgrade pytorch to 1.12\n",
    "#     !pip install /kaggle/input/pytorch112-cu113/{torch-1.12.1+cu113-cp37-cp37m-linux_x86_64.whl,torchvision-0.13.1+cu113-cp37-cp37m-linux_x86_64.whl}\n",
    "#     !pip install /kaggle/input/torch-tensorrt-pkg/nvidia_pyindex-1.0.9-py3-none-any.whl\n",
    "#     !mkdir -p /tmp/pip/cache/\n",
    "#     !cp /kaggle/input/torch-tensorrt-pkg/nvidia-cublas-cu11-2022.4.8.xyz /tmp/pip/cache/nvidia-cublas-cu11-2022.4.8.tar.gz\n",
    "#     !cp /kaggle/input/torch-tensorrt-pkg/nvidia-cuda-runtime-cu11-2022.4.25.xyz /tmp/pip/cache/nvidia-cuda-runtime-cu11-2022.4.25.tar.gz\n",
    "#     !cp /kaggle/input/torch-tensorrt-pkg/nvidia-cudnn-cu11-2022.5.19.xyz /tmp/pip/cache/nvidia-cudnn-cu11-2022.5.19.tar.gz\n",
    "#     !cp /kaggle/input/torch-tensorrt-pkg/nvidia_cublas_cu117-11.10.1.25-py3-none-manylinux1_x86_64.whl /tmp/pip/cache/\n",
    "#     !cp /kaggle/input/torch-tensorrt-pkg/nvidia_cuda_runtime_cu117-11.7.60-py3-none-manylinux1_x86_64.whl /tmp/pip/cache/\n",
    "#     !cp /kaggle/input/torch-tensorrt-pkg/nvidia_cudnn_cu116-8.4.0.27-py3-none-manylinux1_x86_64.whl /tmp/pip/cache/\n",
    "#     !cp /kaggle/input/torch-tensorrt-pkg/nvidia_tensorrt-8.4.3.1-cp37-none-linux_x86_64.whl /tmp/pip/cache/\n",
    "#     !pip install --no-index --find-links /tmp/pip/cache/ nvidia_tensorrt\n",
    "#     #install torch_tensorrt\n",
    "#     !pip install /kaggle/input/torch-tensorrt-pkg/torch_tensorrt-1.2.0-cp37-cp37m-linux_x86_64.whl\n",
    "\n",
    "# try: \n",
    "#     import dicomsdl\n",
    "    \n",
    "# except:\n",
    "#     !pip install /kaggle/input/rsna-2022-whl/pylibjpeg-1.4.0-py3-none-any.whl\n",
    "#     !pip install /kaggle/input/rsna-2022-whl/python_gdcm-3.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "# #     !pip install /kaggle/input/rsna-breast-mammography-00/dicomsdl-0.109.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
    "#     ! pip install dicomsdl\n",
    "\n",
    "# # For NVJPEG2k\n",
    "# !cp /kaggle/input/easy-load-the-image-with-nvjpeg2000/nvjpeg2k.so ./\n",
    "\n",
    "# !pip install --extra-index-url https://developer.download.nvidia.com/compute/redist/nightly nvidia-dali-nightly-cuda110\n",
    "    \n",
    "\n",
    "# import torch_tensorrt\n",
    "# import tensorrt\n",
    "# import torch\n",
    "# print(torch.__version__)\n",
    "\n",
    "# import sys\n",
    "\n",
    "# print('install ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T18:58:29.526080Z",
     "iopub.status.busy": "2023-02-12T18:58:29.525652Z",
     "iopub.status.idle": "2023-02-12T18:58:29.564500Z",
     "shell.execute_reply": "2023-02-12T18:58:29.563039Z",
     "shell.execute_reply.started": "2023-02-12T18:58:29.526044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dicom.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dicom.py\n",
    "# https://github.com/pydicom/pydicom/issues/1554\n",
    "# https://github.com/pydicom/pydicom/issues/539\n",
    "\n",
    "import torch\n",
    "import nvidia.dali as dali\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dicomsdl\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import pydicom\n",
    "import shutil\n",
    "import queue\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method('spawn', force = True)\n",
    "import gc\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut, pixel_dtype\n",
    "# import nvjpeg2k\n",
    "\n",
    "J2K_SYNTAX_UID = '1.2.840.10008.1.2.4.90'\n",
    "J2K_HEADER = b\"\\x00\\x00\\x00\\x0C\"\n",
    "\n",
    "JLOSSLESS_SYNTAX_UID = '1.2.840.10008.1.2.4.70'\n",
    "JLOSSLESS_HEADER = b\"\\xff\\xd8\\xff\\xe0\"\n",
    "\n",
    "def apply_windowing_np(arr,\n",
    "                       window_width=None,\n",
    "                       window_center=None,\n",
    "                       voi_func='LINEAR',\n",
    "                       y_min=0,\n",
    "                       y_max=255):\n",
    "    assert window_width > 0\n",
    "    y_range = y_max - y_min\n",
    "    # float64 needed (default) or just float32 ?\n",
    "    # arr = arr.astype(np.float64)\n",
    "    arr = arr.astype(np.float32)\n",
    "\n",
    "    if voi_func in ['LINEAR', 'LINEAR_EXACT']:\n",
    "        # PS3.3 C.11.2.1.2.1 and C.11.2.1.3.2\n",
    "        if voi_func == 'LINEAR':\n",
    "            if window_width < 1:\n",
    "                raise ValueError(\n",
    "                    \"The (0028,1051) Window Width must be greater than or \"\n",
    "                    \"equal to 1 for a 'LINEAR' windowing operation\")\n",
    "            window_center -= 0.5\n",
    "            window_width -= 1\n",
    "        below = arr <= (window_center - window_width / 2)\n",
    "        above = arr > (window_center + window_width / 2)\n",
    "        between = np.logical_and(~below, ~above)\n",
    "\n",
    "        arr[below] = y_min\n",
    "        arr[above] = y_max\n",
    "        if between.any():\n",
    "            arr[between] = ((\n",
    "                (arr[between] - window_center) / window_width + 0.5) * y_range\n",
    "                            + y_min)\n",
    "    elif voi_func == 'SIGMOID':\n",
    "        arr = y_range / (1 +\n",
    "                         np.exp(-4 *\n",
    "                                (arr - window_center) / window_width)) + y_min\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported (0028,1056) VOI LUT Function value '{voi_func}'\")\n",
    "    return arr\n",
    "\n",
    "\n",
    "def apply_windowing_np_v2(arr,\n",
    "                          window_width=None,\n",
    "                          window_center=None,\n",
    "                          voi_func='LINEAR',\n",
    "                          y_min=0,\n",
    "                          y_max=255):\n",
    "    assert window_width > 0\n",
    "    y_range = y_max - y_min\n",
    "    # float64 needed (default) or just float32 ?\n",
    "    # arr = arr.astype(np.float64)\n",
    "    arr = arr.astype(np.float32)\n",
    "\n",
    "    if voi_func == 'LINEAR' or voi_func == 'LINEAR_EXACT':\n",
    "        # PS3.3 C.11.2.1.2.1 and C.11.2.1.3.2\n",
    "        if voi_func == 'LINEAR':\n",
    "            if window_width < 1:\n",
    "                raise ValueError(\n",
    "                    \"The (0028,1051) Window Width must be greater than or \"\n",
    "                    \"equal to 1 for a 'LINEAR' windowing operation\")\n",
    "            window_center -= 0.5\n",
    "            window_width -= 1\n",
    "\n",
    "        arr = ((arr - window_center) / window_width + 0.5) * y_range + y_min\n",
    "        arr = np.clip(arr, y_min, y_max)\n",
    "\n",
    "    elif voi_func == 'SIGMOID':\n",
    "        arr = y_range / (1 +\n",
    "                         np.exp(-4 *\n",
    "                                (arr - window_center) / window_width)) + y_min\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported (0028,1056) VOI LUT Function value '{voi_func}'\")\n",
    "    return arr\n",
    "\n",
    "\n",
    "def apply_windowing_np_v3(arr,\n",
    "                          window_width=None,\n",
    "                          window_center=None,\n",
    "                          voi_func='LINEAR',\n",
    "                          y_min=0,\n",
    "                          y_max=255):\n",
    "    assert window_width > 0\n",
    "    y_range = y_max - y_min\n",
    "    # float64 needed (default) or just float32 ?\n",
    "    # arr = arr.astype(np.float64)\n",
    "    arr = arr.astype(np.float32)\n",
    "\n",
    "    if voi_func == 'LINEAR' or voi_func == 'LINEAR_EXACT':\n",
    "        # PS3.3 C.11.2.1.2.1 and C.11.2.1.3.2\n",
    "        if voi_func == 'LINEAR':\n",
    "            if window_width < 1:\n",
    "                raise ValueError(\n",
    "                    \"The (0028,1051) Window Width must be greater than or \"\n",
    "                    \"equal to 1 for a 'LINEAR' windowing operation\")\n",
    "            window_center -= 0.5\n",
    "            window_width -= 1\n",
    "        \n",
    "#         arr = ((arr - window_center) / window_width + 0.5) * y_range + y_min\n",
    "        # faster ?\n",
    "        m = y_range / window_width\n",
    "        n = (- window_center / window_width + 0.5) * y_range + y_min\n",
    "        arr = arr * m + n\n",
    "\n",
    "        arr = np.clip(arr, y_min, y_max)\n",
    "\n",
    "    elif voi_func == 'SIGMOID':\n",
    "#         arr = y_range / (1 + np.exp(-4 * (arr - window_center) / window_width)) + y_min\n",
    "        m = -4 / window_width\n",
    "        arr = y_range / (1 + np.exp((arr - window_center) * m) ) + y_min\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported (0028,1056) VOI LUT Function value '{voi_func}'\")\n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "def apply_windowing_torch(arr,\n",
    "                       window_width=None,\n",
    "                       window_center=None,\n",
    "                       voi_func='LINEAR',\n",
    "                       y_min=0,\n",
    "                       y_max=255):\n",
    "    assert window_width > 0\n",
    "    y_range = y_max - y_min\n",
    "    # float64 needed (default) or just float32 ?\n",
    "    # arr = arr.double()\n",
    "    arr = arr.float()\n",
    "\n",
    "    if voi_func == 'LINEAR' or voi_func == 'LINEAR_EXACT':\n",
    "        # PS3.3 C.11.2.1.2.1 and C.11.2.1.3.2\n",
    "        if voi_func == 'LINEAR':\n",
    "            if window_width < 1:\n",
    "                raise ValueError(\n",
    "                    \"The (0028,1051) Window Width must be greater than or \"\n",
    "                    \"equal to 1 for a 'LINEAR' windowing operation\")\n",
    "            window_center -= 0.5\n",
    "            window_width -= 1\n",
    "\n",
    "#         arr = ((arr - window_center) / window_width + 0.5) * y_range + y_min\n",
    "        # faster ?\n",
    "        m = y_range / window_width\n",
    "        n = (- window_center / window_width + 0.5) * y_range + y_min\n",
    "        arr = arr * m + n\n",
    "        arr = torch.clamp(arr, y_min, y_max)\n",
    "\n",
    "    elif voi_func == 'SIGMOID':\n",
    "        m = -4 / window_width\n",
    "        arr = y_range / (1 + torch.exp((arr - window_center) * m) ) + y_min\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported (0028,1056) VOI LUT Function value '{voi_func}'\")\n",
    "    return arr\n",
    "\n",
    "\n",
    "class PydicomMetadata:\n",
    "    def __init__(self, ds):\n",
    "        temp = ds['WindowWidth']\n",
    "        self.window_widths = [float(e) for e in temp] if temp.VM > 1 else [float(temp.value)]\n",
    "        temp = ds['WindowCenter']\n",
    "        self.window_centers = [float(e) for e in temp] if temp.VM > 1 else [float(temp.value)]\n",
    "        # if nan --> LINEAR\n",
    "        self.voilut_func = ds.get('VOILUTFunction', 'LINEAR')\n",
    "\n",
    "        self.invert = (ds.PhotometricInterpretation == 'MONOCHROME1')\n",
    "        self.nbit = ds.BitsStored\n",
    "\n",
    "        assert len(self.window_widths) == len(self.window_widths)\n",
    "\n",
    "\n",
    "class DicomsdlMetadata:\n",
    "    def __init__(self, ds):\n",
    "        self.window_widths = ds.WindowWidth\n",
    "        if not isinstance(self.window_widths, list):\n",
    "            self.window_widths = [self.window_widths]\n",
    "\n",
    "        self.window_centers = ds.WindowCenter\n",
    "        if not isinstance(self.window_centers, list):\n",
    "            self.window_centers = [self.window_centers]\n",
    "        # if nan --> LINEAR\n",
    "        self.voilut_func = ds.VOILUTFunction\n",
    "        self.voilut_func = 'LINEAR' if self.voilut_func is None else self.voilut_func\n",
    "\n",
    "        self.invert = (ds.PhotometricInterpretation == 'MONOCHROME1')\n",
    "        self.nbit = ds.BitsStored\n",
    "\n",
    "        assert len(self.window_widths) == len(self.window_widths)\n",
    "        \n",
    "\n",
    "\n",
    "def min_max_scale(img):\n",
    "    maxv = img.max()\n",
    "    minv = img.min()\n",
    "    if maxv > minv:\n",
    "        return (img - minv) / (maxv - minv)\n",
    "    else:\n",
    "        return img - minv  # ==0\n",
    "\n",
    "\n",
    "def apply_voilut(arr,\n",
    "                  window_width=None,\n",
    "                  window_center=None,\n",
    "                  voi_func='linear',\n",
    "                  y_min=0,\n",
    "                  y_max=255):\n",
    "    assert window_width > 0\n",
    "    y_range = y_max - y_min\n",
    "    # float64 needed or just float32 ?\n",
    "    arr = arr.astype('float64')\n",
    "\n",
    "    if voi_func in ['LINEAR', 'LINEAR_EXACT']:\n",
    "        # PS3.3 C.11.2.1.2.1 and C.11.2.1.3.2\n",
    "        if voi_func == 'LINEAR':\n",
    "            if window_width < 1:\n",
    "                raise ValueError(\n",
    "                    \"The (0028,1051) Window Width must be greater than or \"\n",
    "                    \"equal to 1 for a 'LINEAR' windowing operation\")\n",
    "            window_center -= 0.5\n",
    "            window_width -= 1\n",
    "        below = arr <= (window_center - window_width / 2)\n",
    "        above = arr > (window_center + window_width / 2)\n",
    "        between = np.logical_and(~below, ~above)\n",
    "\n",
    "        arr[below] = y_min\n",
    "        arr[above] = y_max\n",
    "        if between.any():\n",
    "            arr[between] = ((\n",
    "                (arr[between] - window_center) / window_width + 0.5) * y_range\n",
    "                            + y_min)\n",
    "    elif voi_func == 'SIGMOID':\n",
    "        arr = y_range / (1 +\n",
    "                         np.exp(-4 *\n",
    "                                (arr - window_center) / window_width)) + y_min\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported (0028,1056) VOI LUT Function value '{voi_func}'\")\n",
    "    return arr\n",
    "\n",
    "\n",
    "def from_uint16(img, invert=False, dtype='uint8', bit_scale=False, nbit=None):\n",
    "    assert dtype in ['uint8', 'uint16', 'float16', 'float32', 'float64']\n",
    "\n",
    "    if dtype == 'uint16':\n",
    "        if invert:\n",
    "            img = img.max() - img\n",
    "        return img\n",
    "\n",
    "    # first, scaling (uint16 --> float64)\n",
    "    if bit_scale:\n",
    "        maxv = 2**nbit - 1\n",
    "        img = img / maxv\n",
    "    else:\n",
    "        img = min_max_scale(img)\n",
    "\n",
    "    # invert if needed\n",
    "    if invert:\n",
    "        img = 1. - img\n",
    "\n",
    "    # convert to specified dtype\n",
    "    if dtype == 'uint8':\n",
    "        return (img * 255).astype(np.uint8)\n",
    "    else:  # float16/32/64\n",
    "        return img.astype(dtype)\n",
    "\n",
    "\n",
    "def save_img_to_file(save_path, img, backend='cv2'):\n",
    "    file_ext = os.path.basename(save_path).split('.')[-1]\n",
    "    if backend == 'cv2':\n",
    "        if img.dtype == np.uint16:\n",
    "            # https://docs.opencv.org/3.4/d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce\n",
    "            assert file_ext in ['png', 'jp2', 'tiff', 'tif']\n",
    "            cv2.imwrite(save_path, img)\n",
    "        elif img.dtype == np.uint8:\n",
    "            cv2.imwrite(save_path, img)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                '`cv2` backend only support uint8 or uint16 images.')\n",
    "    elif backend == 'np':\n",
    "        assert file_ext == 'npy'\n",
    "        np.save(save_path, img)\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported backend `{backend}`.')\n",
    "\n",
    "\n",
    "def load_img_from_file(img_path, backend='cv2'):\n",
    "    if backend == 'cv2':\n",
    "        return cv2.imread(img_path, cv2.IMREAD_ANYDEPTH)\n",
    "    elif backend == 'np':\n",
    "        return np.load(img_path)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "\n",
    "#https://github.com/NVIDIA/DALI/issues/2330\n",
    "#https://medium.datadriveninvestor.com/gpu-accelerated-data-loading-with-dali-part-2-pipelines-and-data-loaders-99f51548e8a6\n",
    "#https://github.com/NVIDIA/DALI/blob/main/dali/python/nvidia/dali/plugin/pytorch.py\n",
    "#https://zhuanlan.zhihu.com/p/518240063\n",
    "class _DicomToJ2kExternalSourceV1(object):\n",
    "\n",
    "    def __init__(self, j2k_streams):\n",
    "        self.j2k_streams = j2k_streams\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.j2k_streams\n",
    "\n",
    "\n",
    "@dali.pipeline_def\n",
    "def _j2k_decode_pipeline_ram_v1(eii, resize=None):\n",
    "    jpeg = dali.fn.external_source(source=eii,\n",
    "                                   dtype=dali.types.UINT8,\n",
    "                                   batch=True,\n",
    "                                   parallel=False)\n",
    "    image = dali.fn.experimental.decoders.image(\n",
    "        jpeg,\n",
    "        device='mixed',\n",
    "        output_type=dali.types.ANY_DATA,\n",
    "        dtype=dali.types.UINT16)\n",
    "    if resize is not None:\n",
    "        image = dali.fn.resize(image, size=resize)\n",
    "    return image\n",
    "\n",
    "\n",
    "def convert_with_dali_ram_v1(dcm_paths,\n",
    "                             save_paths,\n",
    "                             save_backend='cv2',\n",
    "                             save_dtype='uint8',\n",
    "                             chunk=-1,\n",
    "                             batch_size=16,\n",
    "                             num_threads=2,\n",
    "                             py_num_workers=0,\n",
    "                             device_id=0):\n",
    "    del chunk, py_num_workers\n",
    "    assert len(dcm_paths) == len(save_paths)\n",
    "    assert save_backend in ['cv2', 'np']\n",
    "\n",
    "    num_dcms = len(dcm_paths)\n",
    "\n",
    "    for start_idx in tqdm(range(0, num_dcms, batch_size)):\n",
    "        end_idx = min(num_dcms - 1, start_idx + batch_size)\n",
    "        if end_idx == start_idx:\n",
    "            break\n",
    "        batch_dcm_paths = dcm_paths[start_idx:end_idx]\n",
    "        batch_save_paths = save_paths[start_idx:end_idx]\n",
    "        j2k_streams = []\n",
    "        inverts = []\n",
    "        nbits = []\n",
    "        for dcm_path in batch_dcm_paths:\n",
    "            dcm = pydicom.dcmread(dcm_path)\n",
    "            assert dcm.file_meta.TransferSyntaxUID == J2K_SYNTAX_UID\n",
    "            # # why need this ?\n",
    "            ##################################\n",
    "            # with open(file_path, 'rb') as fp:\n",
    "            #     raw = DicomBytesIO(fp.read())\n",
    "            #     dicom = pydicom.dcmread(raw)\n",
    "            ##################################\n",
    "            pixel_data = dcm.PixelData\n",
    "            offset = pixel_data.find(J2K_HEADER)\n",
    "            # @TODO: different size or not (for each dcm file) ?\n",
    "            # if same size, buffering (np.copyto) for optimization\n",
    "            # @ANS: no, different sizes\n",
    "            j2k_streams.append(\n",
    "                np.array(bytearray(pixel_data[offset:]), np.uint8))\n",
    "            inverts.append(dcm.PhotometricInterpretation == 'MONOCHROME1')\n",
    "            nbits.append(dcm.BitsStored)\n",
    "        # dali to process with chunk in-memory\n",
    "        pipe = _j2k_decode_pipeline_ram_v1(\n",
    "            _DicomToJ2kExternalSourceV1(j2k_streams),\n",
    "            batch_size=len(j2k_streams),\n",
    "            num_threads=num_threads,\n",
    "            device_id=device_id,\n",
    "            debug=False)\n",
    "        pipe.build()\n",
    "        outs = pipe.run()\n",
    "        outs = outs[0].as_cpu()\n",
    "\n",
    "        for i, save_path in enumerate(batch_save_paths):\n",
    "            invert = inverts[i]\n",
    "            nbit = nbits[i]\n",
    "            img = outs.at(i).squeeze(-1)  # uint16\n",
    "            img = from_uint16(img,\n",
    "                              invert,\n",
    "                              save_dtype,\n",
    "                              bit_scale=False,\n",
    "                              nbit=nbit)\n",
    "            save_img_to_file(save_path, img, backend=save_backend)\n",
    "\n",
    "\n",
    "# class _DicomToJ2kExternalSourceV2:\n",
    "\n",
    "#     def __init__(self, dcm_paths):\n",
    "#         self.dcm_paths = dcm_paths\n",
    "#         self.len = len(dcm_paths)\n",
    "\n",
    "#     def __call__(self, sample_info):\n",
    "#         idx = sample_info.idx_in_epoch\n",
    "#         if idx >= self.len:\n",
    "#             raise StopIteration\n",
    "#         # print('IDX:', sample_info.idx_in_epoch, sample_info.idx_in_batch)\n",
    "\n",
    "#         dcm_path = self.dcm_paths[idx]\n",
    "#         dcm = pydicom.dcmread(dcm_path)\n",
    "#         assert dcm.file_meta.TransferSyntaxUID == J2K_SYNTAX_UID\n",
    "#         # # why need this ?\n",
    "#         ##################################\n",
    "#         # with open(file_path, 'rb') as fp:\n",
    "#         #     raw = DicomBytesIO(fp.read())\n",
    "#         #     dicom = pydicom.dcmread(raw)\n",
    "#         ##################################\n",
    "#         pixel_data = dcm.PixelData\n",
    "#         offset = pixel_data.find(J2K_HEADER)\n",
    "#         # @TODO: different size or not (for each dcm file) ?\n",
    "#         # if same size, buffering (np.copyto) for optimization\n",
    "#         # @ANS: no, different sizes\n",
    "#         j2k_stream = np.array(bytearray(pixel_data[offset:]), np.uint8)\n",
    "#         invert = dcm.PhotometricInterpretation == 'MONOCHROME1'\n",
    "#         invert = np.array([invert], dtype=np.bool_)\n",
    "#         return (j2k_stream, invert)\n",
    "\n",
    "# @dali.pipeline_def\n",
    "# def _j2k_decode_pipeline_ram_v2(eii, resize=None, num_outputs=None):\n",
    "#     jpeg, invert = dali.fn.external_source(\n",
    "#         source=eii,\n",
    "#         num_outputs=num_outputs,\n",
    "#         dtype=[dali.types.UINT8, dali.types.BOOL],\n",
    "#         batch=False)\n",
    "#     image = dali.fn.experimental.decoders.image(\n",
    "#         jpeg,\n",
    "#         device='mixed',\n",
    "#         output_type=dali.types.ANY_DATA,\n",
    "#         dtype=dali.types.UINT16)\n",
    "#     if resize is not None:\n",
    "#         image = dali.fn.resize(image, size=resize)\n",
    "#     return image, invert\n",
    "\n",
    "# def convert_with_dali_ram_v2(dcm_paths,\n",
    "#                              save_paths,\n",
    "#                              chunk=64,\n",
    "#                              batch_size = 1,\n",
    "#                              num_threads=2,\n",
    "#                              py_num_workers = 1,\n",
    "#                              device_id=0):\n",
    "#     #@TODO: fix StopIteration\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "#     assert len(dcm_paths) == len(save_paths)\n",
    "\n",
    "#     # dali to process with chunk in-memory\n",
    "#     external_source = _DicomToJ2kExternalSourceV2(dcm_paths)\n",
    "#     pipe = _j2k_decode_pipeline_ram_v2(external_source,\n",
    "#                                       num_outputs=2,\n",
    "#                                       batch_size=16,\n",
    "#                                       num_threads=num_threads,\n",
    "#                                       device_id=device_id,\n",
    "#                                       debug=False)\n",
    "#     pipe.build()\n",
    "\n",
    "#     cur_idx = -1\n",
    "#     while True:\n",
    "#         try:\n",
    "#             outs = pipe.run()\n",
    "#         except StopIteration as e:\n",
    "#             raise e\n",
    "#             break\n",
    "#         print('len:', len(outs[0]))\n",
    "#         imgs = outs[0].as_cpu()\n",
    "#         inverts = outs[1]\n",
    "#         for j in range(len(inverts)):\n",
    "#             cur_idx += 1\n",
    "#             save_path = save_paths[cur_idx]\n",
    "#             img = imgs.at(j).squeeze(-1)\n",
    "#             invert = inverts.at(j)\n",
    "#             img = any_to_fp32(img)\n",
    "#             if invert:\n",
    "#                 img = 1. - img\n",
    "#             img = float_to_uint8(img)\n",
    "#             # print(img.shape)\n",
    "#             # cv2.imwrite(save_path, img)\n",
    "\n",
    "#         print(len(imgs), len(inverts), cur_idx, len(save_paths))\n",
    "\n",
    "#     assert cur_idx == len(\n",
    "#         save_paths) - 1, f'{cur_idx} != {len(save_paths) - 1}'\n",
    "\n",
    "\n",
    "class _DicomToJ2kExternalSourceV3:\n",
    "\n",
    "    def __init__(self, dcm_paths, batch_size=1):\n",
    "        self.dcm_paths = dcm_paths\n",
    "        self.len = len(dcm_paths)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __call__(self, batch_info):\n",
    "        idx = batch_info.iteration\n",
    "        start = idx * self.batch_size\n",
    "        end = min(self.len, start + self.batch_size)\n",
    "        if end <= start:\n",
    "            raise StopIteration()\n",
    "        # print('IDX:', batch_info.iteration, batch_info.epoch_idx)\n",
    "\n",
    "        dcm_paths = self.dcm_paths[start:end]\n",
    "        j2k_streams = []\n",
    "        inverts = []\n",
    "        nbits = []\n",
    "        for dcm_path in dcm_paths:\n",
    "            dcm = pydicom.dcmread(dcm_path)\n",
    "#             assert dcm.file_meta.TransferSyntaxUID == J2K_SYNTAX_UID or \n",
    "#                     dcm.file_meta.TransferSyntaxUID == JLOSSLESS_SYNTAX_UID\n",
    "            # # why need this ?\n",
    "            ##################################\n",
    "            # with open(file_path, 'rb') as fp:\n",
    "            #     raw = DicomBytesIO(fp.read())\n",
    "            #     dicom = pydicom.dcmread(raw)\n",
    "            ##################################\n",
    "            pixel_data = dcm.PixelData\n",
    "            if dcm.file_meta.TransferSyntaxUID == J2K_SYNTAX_UID:\n",
    "                header = J2K_HEADER\n",
    "            elif dcm.file_meta.TransferSyntaxUID == JLOSSLESS_SYNTAX_UID:\n",
    "                header = JLOSSLESS_HEADER\n",
    "            offset = pixel_data.find(header)\n",
    "            # @TODO: different size or not (for each dcm file) ?\n",
    "            # if same size, buffering (np.copyto) for optimization\n",
    "            # @ANS: no, different sizes\n",
    "            j2k_stream = np.array(bytearray(pixel_data[offset:]), np.uint8)\n",
    "            invert = (dcm.PhotometricInterpretation == 'MONOCHROME1')\n",
    "            j2k_streams.append(j2k_stream)\n",
    "            inverts.append(invert)\n",
    "            nbits.append(dcm.BitsStored)\n",
    "        return j2k_streams, np.array(inverts,\n",
    "                                     dtype=np.bool_), np.array(nbits,\n",
    "                                                               dtype=np.uint8)\n",
    "\n",
    "\n",
    "@dali.pipeline_def\n",
    "def _j2k_decode_pipeline_ram_v3(eii, resize=None, num_outputs=None):\n",
    "    jpeg, invert, nbit = dali.fn.external_source(\n",
    "        source=eii,\n",
    "        num_outputs=num_outputs,\n",
    "        dtype=[dali.types.UINT8, dali.types.BOOL, dali.types.UINT8],\n",
    "        batch=True,\n",
    "        batch_info=True,\n",
    "        parallel=True)\n",
    "    image = dali.fn.experimental.decoders.image(\n",
    "        jpeg,\n",
    "        device='mixed',\n",
    "        output_type=dali.types.ANY_DATA,\n",
    "        dtype=dali.types.UINT16)\n",
    "    if resize is not None:\n",
    "        image = dali.fn.resize(image, size=resize)\n",
    "    return image, invert, nbit\n",
    "\n",
    "\n",
    "def convert_with_dali_ram_v3(dcm_paths,\n",
    "                             save_paths,\n",
    "                             save_backend='cv2',\n",
    "                             save_dtype='uint8',\n",
    "                             chunk=-1,\n",
    "                             batch_size=1,\n",
    "                             num_threads=2,\n",
    "                             py_num_workers=1,\n",
    "                             device_id=0):\n",
    "#     import pycuda.autoinit\n",
    "    del chunk\n",
    "    assert len(dcm_paths) == len(save_paths)\n",
    "    assert save_backend in ['cv2', 'np']\n",
    "    num_dcms = len(dcm_paths)\n",
    "\n",
    "    # dali to process with chunk in-memory\n",
    "    external_source = _DicomToJ2kExternalSourceV3(dcm_paths,\n",
    "                                                  batch_size=batch_size)\n",
    "    pipe = _j2k_decode_pipeline_ram_v3(\n",
    "        external_source,\n",
    "        num_outputs=3,\n",
    "        py_num_workers=py_num_workers,\n",
    "        py_start_method = 'spawn',\n",
    "        batch_size=batch_size,\n",
    "        num_threads=num_threads,\n",
    "        device_id=device_id,\n",
    "        debug=False,\n",
    "    )\n",
    "    print('START BUILD!')\n",
    "    try:\n",
    "        pipe.build()\n",
    "    except Exception as e:\n",
    "        print('EXCEPT:', e)\n",
    "    print('BUILD FIRST DONE!')\n",
    "    \n",
    "#     external_source = _DicomToJ2kExternalSourceV3(dcm_paths,\n",
    "#                                                   batch_size=batch_size)\n",
    "#     pipe = _j2k_decode_pipeline_ram_v3(\n",
    "#         external_source,\n",
    "#         num_outputs=3,\n",
    "#         py_num_workers=py_num_workers,\n",
    "#         py_start_method = 'fork',\n",
    "#         batch_size=batch_size,\n",
    "#         num_threads=num_threads,\n",
    "#         device_id=device_id,\n",
    "#         debug=False,\n",
    "#     )\n",
    "#     pipe.start_py_workers()\n",
    "#     pipe.build()\n",
    "    \n",
    "\n",
    "    num_batchs = num_dcms // batch_size\n",
    "    if num_dcms % batch_size > 0:\n",
    "        num_batchs += 1\n",
    "\n",
    "    cur_idx = -1\n",
    "    for _batch_idx in tqdm(range(num_batchs)):\n",
    "        try:\n",
    "            outs = pipe.run()\n",
    "        except StopIteration as e:\n",
    "            raise AssertionError('This should not be the case.')\n",
    "        imgs = outs[0].as_cpu()\n",
    "        inverts = outs[1]\n",
    "        nbits = outs[2]\n",
    "        for j in range(len(inverts)):\n",
    "            cur_idx += 1\n",
    "            save_path = save_paths[cur_idx]\n",
    "            img = imgs.at(j).squeeze(-1)  # uint16\n",
    "            invert = inverts.at(j)\n",
    "            nbit = nbits.at(j)\n",
    "            img = from_uint16(img,\n",
    "                              invert,\n",
    "                              save_dtype,\n",
    "                              bit_scale=False,\n",
    "                              nbit=nbit)\n",
    "            save_img_to_file(save_path, img, backend=save_backend)\n",
    "\n",
    "    assert cur_idx == len(\n",
    "        save_paths) - 1, f'{cur_idx} != {len(save_paths) - 1}'\n",
    "\n",
    "    # del pipe\n",
    "    # gc.collect()\n",
    "\n",
    "\n",
    "@dali.pipeline_def\n",
    "def j2k_decode_pipeline_disk(j2kfiles):\n",
    "    jpegs, _ = dali.fn.readers.file(files=j2kfiles)\n",
    "    images = dali.fn.experimental.decoders.image(\n",
    "        jpegs,\n",
    "        device='mixed',\n",
    "        output_type=dali.types.ANY_DATA,\n",
    "        dtype=dali.types.UINT16)\n",
    "    return images\n",
    "\n",
    "\n",
    "def convert_with_dali_disk(dcm_paths,\n",
    "                           save_paths,\n",
    "                           j2k_temp_dir,\n",
    "                           save_backend='cv2',\n",
    "                           save_dtype='uint8',\n",
    "                           chunk=64,\n",
    "                           batch_size=1,\n",
    "                           num_threads=2,\n",
    "                           py_num_workers=1,\n",
    "                           device_id=0):\n",
    "    del py_num_workers\n",
    "    assert len(dcm_paths) == len(save_paths)\n",
    "    assert save_backend in ['cv2', 'np']\n",
    "\n",
    "    if chunk % batch_size > 0:\n",
    "        print(\n",
    "            'Warning: set chunk divided by batch_size for maximum performance.'\n",
    "        )\n",
    "    num_dcms = len(dcm_paths)\n",
    "\n",
    "    for start_idx in tqdm(range(0, num_dcms, chunk)):\n",
    "        end_idx = min(num_dcms - 1, start_idx + chunk)\n",
    "        if end_idx == start_idx:\n",
    "            break\n",
    "\n",
    "        os.makedirs(j2k_temp_dir, exist_ok=True)\n",
    "        chunk_dcm_paths = dcm_paths[start_idx:end_idx]\n",
    "        chunk_save_paths = save_paths[start_idx:end_idx]\n",
    "\n",
    "        temp_j2k_paths = []\n",
    "        inverts = []\n",
    "        nbits = []\n",
    "        for dcm_path, save_path in zip(chunk_dcm_paths, chunk_save_paths):\n",
    "            dcm = pydicom.dcmread(dcm_path)\n",
    "            assert dcm.file_meta.TransferSyntaxUID == J2K_SYNTAX_UID\n",
    "            # # why need this ?\n",
    "            ##################################\n",
    "            # with open(file_path, 'rb') as fp:\n",
    "            #     raw = DicomBytesIO(fp.read())\n",
    "            #     dicom = pydicom.dcmread(raw)\n",
    "            ##################################\n",
    "            pixel_data = dcm.PixelData\n",
    "            offset = pixel_data.find(J2K_HEADER)\n",
    "            j2k_name = os.path.basename(save_path).replace('.png', '.temp')\n",
    "            temp_j2k_path = os.path.join(j2k_temp_dir, j2k_name)\n",
    "            temp_j2k_paths.append(temp_j2k_path)\n",
    "            with open(temp_j2k_path, \"wb\") as temp_f:\n",
    "                temp_f.write(bytearray(pixel_data[offset:]))\n",
    "            inverts.append(dcm.PhotometricInterpretation == 'MONOCHROME1')\n",
    "            nbits.append(dcm.BitsStored)\n",
    "        # dali to process with chunk in-memory\n",
    "        pipe = j2k_decode_pipeline_disk(temp_j2k_paths,\n",
    "                                        batch_size=batch_size,\n",
    "                                        num_threads=num_threads,\n",
    "                                        device_id=device_id,\n",
    "                                        debug=False)\n",
    "        pipe.build()\n",
    "\n",
    "        chunk_size = len(chunk_dcm_paths)\n",
    "        num_batchs = chunk_size // batch_size\n",
    "        if chunk_size % batch_size > 0:\n",
    "            num_batchs += 1\n",
    "\n",
    "        idx_in_chunk = -1\n",
    "        for _batch_idx in tqdm(range(num_batchs)):\n",
    "            try:\n",
    "                outs = pipe.run()\n",
    "            except StopIteration as e:\n",
    "                raise AssertionError('This should not be the case.')\n",
    "                break\n",
    "            imgs = outs[0].as_cpu()\n",
    "            for j in range(len(imgs)):\n",
    "                idx_in_chunk += 1\n",
    "                save_path = chunk_save_paths[idx_in_chunk]\n",
    "                img = imgs.at(j).squeeze(-1)\n",
    "                invert = inverts[idx_in_chunk]\n",
    "                nbit = nbits[idx_in_chunk]\n",
    "                img = from_uint16(img,\n",
    "                                  invert,\n",
    "                                  save_dtype,\n",
    "                                  bit_scale=False,\n",
    "                                  nbit=nbit)\n",
    "                save_img_to_file(save_path, img, backend=save_backend)\n",
    "\n",
    "        assert idx_in_chunk == chunk_size - 1, f'{idx_in_chunk} != {chunk_size - 1}'\n",
    "\n",
    "        shutil.rmtree(j2k_temp_dir)\n",
    "\n",
    "\n",
    "def convert_with_dali(\n",
    "        dcm_paths,\n",
    "        save_paths,\n",
    "        save_backend='cv2',\n",
    "        save_dtype='uint8',\n",
    "        chunk=64,\n",
    "        batch_size=1,  # for ram_v3 only\n",
    "        num_threads=2,\n",
    "        py_num_workers=1,  # for ram_v3 only\n",
    "        device_id=0,\n",
    "        cache='ram_v3',\n",
    "        j2k_temp_dir=None):\n",
    "    print('CONVERT WITH DALI...')\n",
    "    if cache == 'ram_v1':\n",
    "        convert_with_dali_ram_v1(dcm_paths,\n",
    "                                 save_paths,\n",
    "                                 save_backend=save_backend,\n",
    "                                 save_dtype=save_dtype,\n",
    "                                 chunk=chunk,\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_threads=num_threads,\n",
    "                                 py_num_workers=py_num_workers,\n",
    "                                 device_id=device_id)\n",
    "    # elif cache == 'ram_v2':\n",
    "    #     convert_with_dali_ram_v2(dcm_paths,\n",
    "    #                              save_paths,\n",
    "    #                              chunk=chunk,\n",
    "    #                              num_threads=num_threads,\n",
    "    #                              device_id=device_id)\n",
    "    elif cache == 'ram_v3':\n",
    "        convert_with_dali_ram_v3(dcm_paths,\n",
    "                                 save_paths,\n",
    "                                 save_backend=save_backend,\n",
    "                                 save_dtype=save_dtype,\n",
    "                                 chunk=chunk,\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_threads=num_threads,\n",
    "                                 py_num_workers=py_num_workers,\n",
    "                                 device_id=device_id)\n",
    "    elif cache == 'disk':\n",
    "        assert j2k_temp_dir is not None\n",
    "        convert_with_dali_disk(dcm_paths,\n",
    "                               save_paths,\n",
    "                               j2k_temp_dir,\n",
    "                               save_backend=save_backend,\n",
    "                               save_dtype=save_dtype,\n",
    "                               chunk=chunk,\n",
    "                               batch_size=batch_size,\n",
    "                               num_threads=num_threads,\n",
    "                               py_num_workers=py_num_workers,\n",
    "                               device_id=device_id)\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported cache method {cache}')\n",
    "\n",
    "\n",
    "def convert_with_dali_parallel(\n",
    "        dcm_paths,\n",
    "        save_paths,\n",
    "        save_backend='cv2',\n",
    "        save_dtype='uint8',\n",
    "        chunk=64,\n",
    "        batch_size=1,  # for ram_v3 only\n",
    "        num_threads=2,\n",
    "        py_num_workers=1,  # for ram_v3 only\n",
    "        device_id=0,\n",
    "        cache='ram_v3',\n",
    "        j2k_temp_dir=None,\n",
    "        parallel_n_jobs=2,\n",
    "        parallel_backend='loky'):\n",
    "    if parallel_n_jobs == 1:\n",
    "        print('No parralel. Starting the tasks within current process.')\n",
    "        return convert_with_dali(dcm_paths,\n",
    "                                 save_paths,\n",
    "                                 save_backend=save_backend,\n",
    "                                 save_dtype=save_dtype,\n",
    "                                 chunk=chunk,\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_threads=num_threads,\n",
    "                                 py_num_workers=py_num_workers,\n",
    "                                 device_id=device_id,\n",
    "                                 cache=cache,\n",
    "                                 j2k_temp_dir=j2k_temp_dir)\n",
    "    \n",
    "#         _ = Parallel(n_jobs=parallel_n_jobs, backend=parallel_backend)(\n",
    "#             delayed(convert_with_dali)(\n",
    "#                 dcm_paths,\n",
    "#                 save_paths,\n",
    "#                 save_backend=save_backend,\n",
    "#                 save_dtype=save_dtype,\n",
    "#                 chunk=chunk,  # disk\n",
    "#                 batch_size=batch_size,  # disk, ram_v3\n",
    "#                 num_threads=num_threads,\n",
    "#                 py_num_workers=py_num_workers,  # ram_v3\n",
    "#                 device_id=device_id,\n",
    "#                 cache=cache,\n",
    "#                 j2k_temp_dir=j2k_temp_dir,  # disk)\n",
    "#             ) for i in range(1))\n",
    "    else:\n",
    "        assert cache != 'disk', 'Cache method `disk` can not be used in parallel.'\n",
    "        num_samples = len(dcm_paths)\n",
    "        num_samples_per_worker = num_samples // parallel_n_jobs\n",
    "        if num_samples % parallel_n_jobs > 0:\n",
    "            num_samples_per_worker += 1\n",
    "        starts = [num_samples_per_worker * i for i in range(parallel_n_jobs)]\n",
    "        ends = [\n",
    "            min(start + num_samples_per_worker, num_samples)\n",
    "            for start in starts\n",
    "        ]\n",
    "        if isinstance(device_id, list):\n",
    "            assert len(device_id) == parallel_n_jobs\n",
    "        elif isinstance(device_id, int):\n",
    "            device_id = [device_id] * parallel_n_jobs\n",
    "\n",
    "        \n",
    "        print(\n",
    "            f'Starting {parallel_n_jobs} jobs with backend `{parallel_backend}`...'\n",
    "        )\n",
    "#         _ = Parallel(n_jobs=parallel_n_jobs, backend=parallel_backend)(\n",
    "#             delayed(convert_with_dali)(\n",
    "#                 dcm_paths[start:end],\n",
    "#                 save_paths[start:end],\n",
    "#                 save_backend=save_backend,\n",
    "#                 save_dtype=save_dtype,\n",
    "#                 chunk=chunk,  # disk\n",
    "#                 batch_size=batch_size,  # disk, ram_v3\n",
    "#                 num_threads=num_threads,\n",
    "#                 py_num_workers=py_num_workers,  # ram_v3\n",
    "#                 device_id=worker_device_id,\n",
    "#                 cache=cache,\n",
    "#                 j2k_temp_dir=j2k_temp_dir,  # disk)\n",
    "#             ) for start, end, worker_device_id in zip(starts, ends, device_id))\n",
    "\n",
    "        \n",
    "        workers = []\n",
    "        for i in range(parallel_n_jobs):\n",
    "            start = starts[i]\n",
    "            end = ends[i]\n",
    "            worker_device_id = device_id[i]\n",
    "            worker = multiprocessing.Process(group = None,\n",
    "                                             target = convert_with_dali,\n",
    "                                            args = (dcm_paths[start:end], save_paths[start:end],),\n",
    "                                            kwargs = {\n",
    "                                                'save_backend': save_backend,\n",
    "                                                'save_dtype': save_dtype,\n",
    "                                                'chunk': chunk,\n",
    "                                                'batch_size': batch_size,\n",
    "                                                'num_threads': num_threads,\n",
    "                                                'py_num_workers': py_num_workers,\n",
    "                                                'device_id': worker_device_id,\n",
    "                                                'cache': cache,\n",
    "                                                'j2k_temp_dir': j2k_temp_dir,\n",
    "                                            },\n",
    "                                            daemon = False)\n",
    "            workers.append(worker)\n",
    "            \n",
    "        for worker in workers:\n",
    "            worker.start()\n",
    "            print('Start worker!')\n",
    "        for worker in workers:\n",
    "            worker.join()\n",
    "\n",
    "\n",
    "def load_img_dicomsdl(dcm_path, dtype='uint8', index=0, voilut = True):\n",
    "    dcm = dicomsdl.open(dcm_path)\n",
    "    info = dcm.getPixelDataInfo()\n",
    "    ori_dtype = info['dtype']\n",
    "    if info['SamplesPerPixel'] != 1:\n",
    "        raise RuntimeError('SamplesPerPixel != 1')\n",
    "    else:\n",
    "        shape = [info['Rows'], info['Cols']]\n",
    "    img = np.empty(shape, dtype=ori_dtype)\n",
    "    assert img.dtype == np.uint16, f'{img.dtype}'\n",
    "    dcm.copyFrameData(index, img)\n",
    "    \n",
    "    metadata = DicomsdlMetadata(dcm)\n",
    "    if len(metadata.window_centers) == 0:\n",
    "        print('No windows')\n",
    "        voilut = False\n",
    "\n",
    "    if voilut:\n",
    "        assert dtype in ['uint8']\n",
    "        \n",
    "        # np v1\n",
    "        start = time.time()\n",
    "        img_np_v1 = apply_windowing_np(img,\n",
    "            window_width=metadata.window_widths[0],\n",
    "            window_center=metadata.window_centers[0],\n",
    "            voi_func=metadata.voilut_func,\n",
    "            y_min=0,\n",
    "            y_max=255)\n",
    "        end = time.time()\n",
    "        take_np_v1 = round((end - start) * 1000, 2)\n",
    "        \n",
    "        # np v2\n",
    "        start = time.time()\n",
    "        img_np_v2 = apply_windowing_np_v2(img,\n",
    "            window_width=metadata.window_widths[0],\n",
    "            window_center=metadata.window_centers[0],\n",
    "            voi_func=metadata.voilut_func,\n",
    "            y_min=0,\n",
    "            y_max=255)\n",
    "        end = time.time()\n",
    "        take_np_v2 = round((end - start) * 1000, 2)\n",
    "        \n",
    "        \n",
    "        # np v2\n",
    "        start = time.time()\n",
    "        img_np_v3 = apply_windowing_np_v3(img,\n",
    "            window_width=metadata.window_widths[0],\n",
    "            window_center=metadata.window_centers[0],\n",
    "            voi_func=metadata.voilut_func,\n",
    "            y_min=0,\n",
    "            y_max=255)\n",
    "        end = time.time()\n",
    "        take_np_v3 = round((end - start) * 1000, 2)\n",
    "        \n",
    "        \n",
    "        img2 = np.empty(shape, dtype=np.int16)\n",
    "        dcm.copyFrameData(index, img2)\n",
    "        img2 = torch.from_numpy(img2).cuda()\n",
    "    \n",
    "        # torch\n",
    "        t1 = time.time()\n",
    "        img_torch = torch.from_numpy(img.astype(np.int16)).cuda()\n",
    "        diff4 = torch.max(torch.abs(img_torch - img2))\n",
    "        assert diff4.cpu().numpy() == 0\n",
    "        print('???', diff4)\n",
    "        t2 = time.time()\n",
    "        img_torch = apply_windowing_torch(img_torch,\n",
    "            window_width=metadata.window_widths[0],\n",
    "            window_center=metadata.window_centers[0],\n",
    "            voi_func=metadata.voilut_func,\n",
    "            y_min=0,\n",
    "            y_max=255)\n",
    "        t3 = time.time()\n",
    "        img_torch = img_torch.cpu().numpy()\n",
    "        t4 = time.time()\n",
    "        take_torch = f'[{round((t2 - t1) * 1000, 2)} {round((t3 - t2) * 1000, 2)} {round((t4 - t3) * 1000, 2)} {round((t4 - t1) * 1000, 2)}]'\n",
    "        \n",
    "        diff1, diff2, diff3 = np.max(np.abs(img_np_v1 - img_np_v2)), np.max(np.abs(img_np_v2 - img_np_v3)), np.max(np.abs(img_np_v2 - img_torch))\n",
    "        \n",
    "        print(f'{metadata.voilut_func} {diff1} {diff2} {diff3} with time np_v1 = {take_np_v1}, np_v2 = {take_np_v2}, np_v3 = {take_np_v3}, torch = {take_torch}')\n",
    "        \n",
    "        img = img_torch\n",
    "        if metadata.invert:\n",
    "            img = 255 - img\n",
    "        img = img.astype(np.uint8)\n",
    "    else:\n",
    "        img = from_uint16(img, metadata.invert, dtype, bit_scale=False)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _convert_single_with_dicomsdl(dcm_path,\n",
    "                                  save_path,\n",
    "                                  save_backend='cv2',\n",
    "                                  save_dtype='uint8'):\n",
    "    img = load_img_dicomsdl(dcm_path, dtype=save_dtype)\n",
    "    save_img_to_file(save_path, img, backend=save_backend)\n",
    "\n",
    "\n",
    "def convert_with_dicomsdl(dcm_paths,\n",
    "                          save_paths,\n",
    "                          save_backend='cv2',\n",
    "                          save_dtype='uint8'):\n",
    "    assert len(dcm_paths) == len(save_paths)\n",
    "    for dcm_path, save_path in tqdm(zip(dcm_paths, save_paths)):\n",
    "        _convert_single_with_dicomsdl(dcm_path, save_path, save_backend,\n",
    "                                      save_dtype)\n",
    "\n",
    "\n",
    "def convert_with_dicomsdl_parallel(dcm_paths,\n",
    "                                   save_paths,\n",
    "                                   save_backend='cv2',\n",
    "                                   save_dtype='uint8',\n",
    "                                   parallel_n_jobs=2,\n",
    "                                   parallel_backend='loky'):\n",
    "    assert len(dcm_paths) == len(save_paths)\n",
    "    if parallel_n_jobs == 1:\n",
    "        print('No parralel. Starting the tasks within current process.')\n",
    "        return convert_with_dicomsdl(dcm_paths, save_paths, save_backend,\n",
    "                                     save_dtype)\n",
    "    else:\n",
    "        print(\n",
    "            f'Starting {parallel_n_jobs} jobs with backend `{parallel_backend}`...'\n",
    "        )\n",
    "        _ = Parallel(n_jobs=parallel_n_jobs, backend=parallel_backend)(\n",
    "            delayed(_convert_single_with_dicomsdl)(dcm_paths[j], save_paths[j],\n",
    "                                                   save_backend, save_dtype)\n",
    "            for j in tqdm(range(len(dcm_paths))))\n",
    "\n",
    "\n",
    "def load_img_pydicom(dcm_path, dtype='uint8'):\n",
    "    dcm = pydicom.dcmread(dcm_path)\n",
    "    img = dcm.pixel_array\n",
    "    # print(dcm.BitsStored, dcm.BitsAllocated, img.dtype, img.min(), img.max())\n",
    "    assert img.dtype == np.uint16, f'{img.dtype}'\n",
    "    invert = dcm.PhotometricInterpretation == 'MONOCHROME1'\n",
    "    img = from_uint16(img, invert, dtype, bit_scale=False)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _convert_single_with_pydicom(dcm_path,\n",
    "                                 save_path,\n",
    "                                 save_backend='cv2',\n",
    "                                 save_dtype='uint8'):\n",
    "    img = load_img_pydicom(dcm_path, dtype=save_dtype)\n",
    "    save_img_to_file(save_path, img, backend=save_backend)\n",
    "\n",
    "\n",
    "def convert_with_pydicom(dcm_paths,\n",
    "                         save_paths,\n",
    "                         save_backend='cv2',\n",
    "                         save_dtype='uint8'):\n",
    "    assert len(dcm_paths) == len(save_paths)\n",
    "    for dcm_path, save_path in tqdm(zip(dcm_paths, save_paths)):\n",
    "        _convert_single_with_pydicom(dcm_path, save_path, save_backend,\n",
    "                                     save_dtype)\n",
    "\n",
    "\n",
    "def convert_with_pydicom_parallel(dcm_paths,\n",
    "                                  save_paths,\n",
    "                                  save_backend='cv2',\n",
    "                                  save_dtype='uint8',\n",
    "                                  parallel_n_jobs=2,\n",
    "                                  parallel_backend='loky'):\n",
    "    assert len(dcm_paths) == len(save_paths)\n",
    "    if parallel_n_jobs == 1:\n",
    "        print('No parralel. Starting the tasks within current process.')\n",
    "        return convert_with_pydicom(dcm_paths, save_paths, save_backend,\n",
    "                                    save_dtype)\n",
    "    else:\n",
    "        print(\n",
    "            f'Starting {parallel_n_jobs} jobs with backend `{parallel_backend}`...'\n",
    "        )\n",
    "        _ = Parallel(n_jobs=parallel_n_jobs, backend=parallel_backend)(\n",
    "            delayed(_convert_single_with_pydicom)(dcm_paths[j], save_paths[j],\n",
    "                                                  save_backend, save_dtype)\n",
    "            for j in tqdm(range(len(dcm_paths))))\n",
    "        \n",
    "############################################        \n",
    "##### NVJPEG2K\n",
    "\n",
    "from nvidia.dali.experimental import eager\n",
    "import nvidia.dali.types as types\n",
    "from nvidia.dali.types import DALIDataType\n",
    "        \n",
    "def load_img_nvjpeg2k(j2k_decoder, dcm_path, dtype='uint8'):\n",
    "    dcm = pydicom.dcmread(dcm_path)\n",
    "    metadata  = PydicomMetadata(dcm)\n",
    "#     assert dcm.file_meta.TransferSyntaxUID == J2K_SYNTAX_UID\n",
    "    pixel_data = dcm.PixelData\n",
    "    \n",
    "    if dcm.file_meta.TransferSyntaxUID == J2K_SYNTAX_UID:\n",
    "        header = J2K_HEADER\n",
    "    elif dcm.file_meta.TransferSyntaxUID == JLOSSLESS_SYNTAX_UID:\n",
    "        header = JLOSSLESS_HEADER\n",
    "    offset = pixel_data.find(header)\n",
    "    j2k_stream = bytearray(pixel_data[offset:])\n",
    "    \n",
    "#     img = j2k_decoder.decode(j2k_stream)\n",
    "\n",
    "    j2k_stream = np.array(j2k_stream, dtype=np.uint8)\n",
    "    output = eager.experimental.decoders.image([j2k_stream],\n",
    "                                              device='gpu',\n",
    "                                              output_type=types.ANY_DATA,\n",
    "                                              dtype=DALIDataType.UINT16)\n",
    "    img = output.as_cpu().at(0).squeeze(-1)\n",
    "    \n",
    "    assert img.dtype == np.uint16, f'Dtype is not uint16: {img.dtype}'\n",
    "    invert = dcm.PhotometricInterpretation == 'MONOCHROME1'\n",
    "    img = from_uint16(img, invert, dtype, bit_scale=False)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _convert_single_with_nvjpeg2k(j2k_decoder,\n",
    "                                dcm_path,\n",
    "                                 save_path,\n",
    "                                 save_backend='cv2',\n",
    "                                 save_dtype='uint8'):\n",
    "    img = load_img_nvjpeg2k(j2k_decoder, dcm_path, dtype=save_dtype)\n",
    "    save_img_to_file(save_path, img, backend=save_backend)\n",
    "\n",
    "\n",
    "def convert_with_nvjpeg2k(dcm_paths,\n",
    "                         save_paths,\n",
    "                         save_backend='cv2',\n",
    "                         save_dtype='uint8'):\n",
    "    assert len(dcm_paths) == len(save_paths)\n",
    "#     j2k_decoder = nvjpeg2k.Decoder()\n",
    "    j2k_decoder = None\n",
    "    for i in tqdm(range(len(dcm_paths))):\n",
    "        _convert_single_with_nvjpeg2k(j2k_decoder, dcm_paths[i], save_paths[i], save_backend,\n",
    "                                     save_dtype)\n",
    "\n",
    "\n",
    "def convert_with_nvjpeg2k_parallel(dcm_paths,\n",
    "                                  save_paths,\n",
    "                                  save_backend='cv2',\n",
    "                                  save_dtype='uint8',\n",
    "                                  parallel_n_jobs=2,\n",
    "                                  parallel_backend='loky'):\n",
    "    assert len(dcm_paths) == len(save_paths)\n",
    "    if parallel_n_jobs == 1:\n",
    "        print('No parralel. Starting the tasks within current process.')\n",
    "        return convert_with_nvjpeg2k(dcm_paths, save_paths, save_backend,\n",
    "                                    save_dtype)\n",
    "    else:\n",
    "        print(\n",
    "            f'Starting {parallel_n_jobs} jobs with backend `{parallel_backend}`...'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        num_samples = len(dcm_paths)\n",
    "        num_samples_per_worker = num_samples // parallel_n_jobs\n",
    "        if num_samples % parallel_n_jobs > 0:\n",
    "            num_samples_per_worker += 1\n",
    "        starts = [num_samples_per_worker * i for i in range(parallel_n_jobs)]\n",
    "        ends = [\n",
    "            min(start + num_samples_per_worker, num_samples)\n",
    "            for start in starts\n",
    "        ]        \n",
    "        \n",
    "        _ = Parallel(n_jobs=parallel_n_jobs, backend=parallel_backend)(\n",
    "            delayed(convert_with_nvjpeg2k)(dcm_paths[start:end],\n",
    "                                           save_paths[start:end],\n",
    "                                           save_backend,\n",
    "                                           save_dtype)\n",
    "            for start, end in zip(starts, ends))\n",
    "        \n",
    "\n",
    "\n",
    "def make_uid_transfer_dict(df, dcm_root_dir):\n",
    "    machine_id_to_transfer = {}\n",
    "    machine_id = df.machine_id.unique()\n",
    "    for i in machine_id:\n",
    "        row = df[df.machine_id == i].iloc[0]\n",
    "        sample_dcm_path = os.path.join(dcm_root_dir, str(row.patient_id),\n",
    "                                       f'{row.image_id}.dcm')\n",
    "        dicom = pydicom.dcmread(sample_dcm_path)\n",
    "        machine_id_to_transfer[i] = dicom.file_meta.TransferSyntaxUID\n",
    "    return machine_id_to_transfer\n",
    "\n",
    "\n",
    "def convert_all(\n",
    "    df,\n",
    "    dcm_root_dir,\n",
    "    save_root_dir,\n",
    "    dicomsdl_num_processes=2,\n",
    "):\n",
    "    os.makedirs(save_root_dir, exist_ok=True)\n",
    "    machine_id_to_syntax_uid = make_uid_transfer_dict(df, dcm_root_dir)\n",
    "    dali_dcm_paths = []\n",
    "    dali_save_paths = []\n",
    "    dicomsdl_dcm_paths = []\n",
    "    dicomsdl_save_paths = []\n",
    "    for i, row in df.iterrows():\n",
    "        dcm_path = os.path.join(dcm_root_dir, str(row.patient_id),\n",
    "                                f'{row.image_id}.dcm')\n",
    "        save_path = os.path.join(save_root_dir,\n",
    "                                 f'{row.patient_id}@{row.image_id}.png')\n",
    "        syntax_uid = machine_id_to_syntax_uid[row.machine_id]\n",
    "        if syntax_uid == J2K_SYNTAX_UID:\n",
    "            dali_dcm_paths.append(dcm_path)\n",
    "            dali_save_paths.append(save_path)\n",
    "        else:\n",
    "            dicomsdl_dcm_paths.append(dcm_path)\n",
    "            dicomsdl_save_paths.append(save_path)\n",
    "\n",
    "    # process with dali\n",
    "    print('Convert with DALI:', len(dali_dcm_paths))\n",
    "    start = time.time()\n",
    "    j2k_temp_dir = os.path.join(save_root_dir, 'temp')\n",
    "    convert_with_dali(\n",
    "        dali_dcm_paths,\n",
    "        dali_save_paths,\n",
    "        chunk=64,  # disk\n",
    "        batch_size=1,  # disk, ram_v3\n",
    "        num_threads=2,\n",
    "        py_num_workers=1,  # ram_v3\n",
    "        device_id=0,\n",
    "        cache='ram_v3',\n",
    "        j2k_temp_dir=j2k_temp_dir,  # disk\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    print(f'\\n---DALI done in {end - start} sec.\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class _Tester:\n",
    "\n",
    "    def __init__(self):\n",
    "        csv_path = '../../datasets/train.csv'\n",
    "        dcm_root_dir = '../../datasets/train_images/'\n",
    "        save_root_dir = 'temp_save'\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        from sklearn.utils import shuffle\n",
    "        df = shuffle(df, random_state = 42).reset_index(drop = True)\n",
    "        df = df[:2000]\n",
    "        print('Total samples:', len(df))\n",
    "        \n",
    "        os.makedirs(save_root_dir, exist_ok=True)\n",
    "\n",
    "        machine_id_to_syntax_uid = make_uid_transfer_dict(df, dcm_root_dir)\n",
    "        dali_dcm_paths = []\n",
    "        dali_save_paths = []\n",
    "        dicomsdl_dcm_paths = []\n",
    "        dicomsdl_save_paths = []\n",
    "        for i, row in df.iterrows():\n",
    "            dcm_path = os.path.join(dcm_root_dir, str(row.patient_id),\n",
    "                                    f'{row.image_id}.dcm')\n",
    "            save_path = os.path.join(save_root_dir,\n",
    "                                     f'{row.patient_id}@{row.image_id}')\n",
    "            syntax_uid = machine_id_to_syntax_uid[row.machine_id]\n",
    "            if syntax_uid == J2K_SYNTAX_UID:\n",
    "                dali_dcm_paths.append(dcm_path)\n",
    "                dali_save_paths.append(save_path)\n",
    "            else:\n",
    "                dicomsdl_dcm_paths.append(dcm_path)\n",
    "                dicomsdl_save_paths.append(save_path)\n",
    "\n",
    "        self.save_root_dir = save_root_dir\n",
    "        self.dali_dcm_paths = dali_dcm_paths\n",
    "        self.dali_save_paths = dali_save_paths\n",
    "        self.dicomsdl_dcm_paths = dicomsdl_dcm_paths\n",
    "        self.dicomsdl_save_paths = dicomsdl_save_paths\n",
    "\n",
    "    def speed_compare(self):\n",
    "        dcm_paths = self.dali_dcm_paths\n",
    "        save_paths = self.dali_save_paths\n",
    "        save_root_dir = self.save_root_dir\n",
    "\n",
    "        ########### PROCESS WITH DALI\n",
    "        print('Convert with DALI:', len(dcm_paths))\n",
    "        start = time.time()\n",
    "        j2k_temp_dir = os.path.join(save_root_dir, 'temp')\n",
    "        convert_with_dali_parallel(\n",
    "            dcm_paths,\n",
    "            save_paths,\n",
    "            chunk=64,  # disk\n",
    "            batch_size=1,  # disk, ram_v3\n",
    "            num_threads=2,\n",
    "            py_num_workers=1,  # ram_v3\n",
    "            device_id=0,\n",
    "            cache='ram_v3',\n",
    "            j2k_temp_dir=j2k_temp_dir,  # disk\n",
    "            parallel_n_jobs=4,\n",
    "            parallel_backend='loky')\n",
    "        end = time.time()\n",
    "        print(f'\\n---DALI done in {end - start} sec.\\n')\n",
    "\n",
    "        ############ PROCESS WITH DICOMSDL\n",
    "        dicomsdl_dcm_paths = dcm_paths\n",
    "        dicomsdl_save_paths = save_paths\n",
    "        print('Convert with dicomsdl:', len(dicomsdl_dcm_paths))\n",
    "        start = time.time()\n",
    "        convert_with_dicomsdl_parallel(dicomsdl_dcm_paths,\n",
    "                                       dicomsdl_save_paths,\n",
    "                                       parallel_n_jobs=4,\n",
    "                                       parallel_backend='loky')\n",
    "        end = time.time()\n",
    "        print(f'\\n---Dicomsdl done in {end - start} sec.\\n')\n",
    "\n",
    "    def dali_parallel(self):\n",
    "        dcm_paths = self.dali_dcm_paths\n",
    "        save_paths = self.dali_save_paths\n",
    "        save_root_dir = self.save_root_dir\n",
    "\n",
    "        # process with dali\n",
    "        print('Convert with DALI:', len(dcm_paths))\n",
    "        start = time.time()\n",
    "        j2k_temp_dir = os.path.join(save_root_dir, 'temp')\n",
    "        convert_with_dali_parallel(\n",
    "            dcm_paths,\n",
    "            save_paths,\n",
    "            chunk=64,  # disk\n",
    "            batch_size=1,  # disk, ram_v3\n",
    "            num_threads=2,\n",
    "            py_num_workers=1,  # ram_v3\n",
    "            device_id=0,\n",
    "            cache='ram_v3',\n",
    "            j2k_temp_dir=j2k_temp_dir,  # disk\n",
    "            parallel_n_jobs=4,\n",
    "            parallel_backend='loky')\n",
    "        end = time.time()\n",
    "        print(f'\\n---DALI done in {end - start} sec.\\n')\n",
    "\n",
    "    def dicomsdl_parallel(self):\n",
    "        dcm_paths = self.dali_dcm_paths\n",
    "        save_paths = self.dali_save_paths\n",
    "\n",
    "        ############ PROCESS WITH DICOMSDL\n",
    "        dicomsdl_dcm_paths = dcm_paths\n",
    "        dicomsdl_save_paths = save_paths\n",
    "        print('Convert with dicomsdl:', len(dicomsdl_dcm_paths))\n",
    "        start = time.time()\n",
    "        convert_with_dicomsdl_parallel(dicomsdl_dcm_paths,\n",
    "                                       dicomsdl_save_paths,\n",
    "                                       parallel_n_jobs=4,\n",
    "                                       parallel_backend='loky')\n",
    "        end = time.time()\n",
    "        print(f'\\n---Dicomsdl done in {end - start} sec.\\n')\n",
    "\n",
    "    def compare_results(self, save_backend='cv2', save_dtype='uint8'):\n",
    "        if save_backend == 'cv2':\n",
    "            file_ext = 'png'\n",
    "        elif save_backend == 'np':\n",
    "            file_ext = 'npy'\n",
    "            \n",
    "#         dcm_paths = self.dali_dcm_paths[:50]\n",
    "#         save_paths = self.dali_save_paths[:50]\n",
    "        \n",
    "        dcm_paths = self.dicomsdl_dcm_paths[:500]\n",
    "        save_paths = self.dicomsdl_save_paths[:500]\n",
    "\n",
    "        dali_dcm_paths = dcm_paths\n",
    "        dicomsdl_dcm_paths = dcm_paths\n",
    "        pydicom_dcm_paths = dcm_paths\n",
    "        nvjpeg_dcm_paths = dcm_paths\n",
    "        dali_save_paths = [\n",
    "            f'{p}_{save_dtype}_dali.{file_ext}' for p in save_paths\n",
    "        ]\n",
    "        nvjpeg_save_paths = [\n",
    "            f'{p}_{save_dtype}_nvjpeg.{file_ext}' for p in save_paths\n",
    "        ]\n",
    "        dicomsdl_save_paths = [\n",
    "            f'{p}_{save_dtype}_dicomsdl.{file_ext}' for p in save_paths\n",
    "        ]\n",
    "        pydicom_save_paths = [\n",
    "            f'{p}_{save_dtype}_pydicom.{file_ext}' for p in save_paths\n",
    "        ]\n",
    "        save_root_dir = self.save_root_dir\n",
    "\n",
    "#         # process with dali\n",
    "#         print('Convert with DALI:', len(dali_dcm_paths))\n",
    "#         start = time.time()\n",
    "#         j2k_temp_dir = os.path.join(save_root_dir, 'temp')\n",
    "#         convert_with_dali_parallel(\n",
    "#             dali_dcm_paths,\n",
    "#             dali_save_paths,\n",
    "#             save_backend = save_backend,\n",
    "#             save_dtype = save_dtype,\n",
    "#             chunk=64,  # disk\n",
    "#             batch_size=1,  # disk, ram_v3\n",
    "#             num_threads=1,\n",
    "#             py_num_workers=1,  # ram_v3\n",
    "#             device_id=0,\n",
    "#             cache='ram_v3',\n",
    "#             j2k_temp_dir=j2k_temp_dir,  # disk\n",
    "#             parallel_n_jobs=4,\n",
    "#             parallel_backend='multiprocessing')\n",
    "#         end = time.time()\n",
    "#         print(f'\\n---DALI done in {end - start} sec.\\n')\n",
    "        \n",
    "        \n",
    "#         #### CONVERT WITH NVJPEG\n",
    "#         print('Convert with nvjpeg2k:', len(nvjpeg_dcm_paths))\n",
    "#         start = time.time()\n",
    "#         convert_with_nvjpeg2k_parallel(nvjpeg_dcm_paths,\n",
    "#                                        nvjpeg_save_paths,\n",
    "#                                        save_backend = save_backend,\n",
    "#                                        save_dtype = save_dtype,\n",
    "#                                        parallel_n_jobs=4,\n",
    "#                                        parallel_backend='loky')\n",
    "#         end = time.time()\n",
    "#         print(f'\\n---NVJPEG2K done in {end - start} sec.\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        ############ PROCESS WITH DICOMSDL\n",
    "        dicomsdl_dcm_paths = dcm_paths\n",
    "        print('Convert with dicomsdl:', len(dicomsdl_dcm_paths))\n",
    "        start = time.time()\n",
    "        convert_with_dicomsdl_parallel(dicomsdl_dcm_paths,\n",
    "                                       dicomsdl_save_paths,\n",
    "                                       save_backend = save_backend,\n",
    "                                       save_dtype = save_dtype,\n",
    "                                       parallel_n_jobs=1,\n",
    "                                       parallel_backend='loky')\n",
    "        end = time.time()\n",
    "        print(f'\\n---Dicomsdl done in {end - start} sec.\\n')\n",
    "\n",
    "        ############ PROCESS WITH PYDICOM\n",
    "        \n",
    "        print('Convert with pydicom:', len(pydicom_dcm_paths))\n",
    "        start = time.time()\n",
    "        convert_with_pydicom_parallel(pydicom_dcm_paths,\n",
    "                                       pydicom_save_paths,\n",
    "                                       save_backend = save_backend,\n",
    "                                       save_dtype = save_dtype,\n",
    "                                       parallel_n_jobs=4,\n",
    "                                       parallel_backend='loky')\n",
    "        end = time.time()\n",
    "        print(f'\\n---Pydicom done in {end - start} sec.\\n')\n",
    "\n",
    "        print('Compare results:')\n",
    "        for dali_save_path, nvjpeg_save_path, dicomsdl_save_path, pydicom_save_path in zip(\n",
    "                dali_save_paths, nvjpeg_save_paths, dicomsdl_save_paths, pydicom_save_paths):\n",
    "            print(dicomsdl_save_path)\n",
    "            dali_img = load_img_from_file(dali_save_path, backend=save_backend)\n",
    "            nvjpeg_img = load_img_from_file(nvjpeg_save_path, backend=save_backend)\n",
    "            dicomsdl_img = load_img_from_file(dicomsdl_save_path,\n",
    "                                              backend=save_backend)\n",
    "            pydicom_img = load_img_from_file(pydicom_save_path,\n",
    "                                             backend=save_backend)\n",
    "            if dali_img is None or nvjpeg_img is None or dicomsdl_img is None or pydicom_img is None:\n",
    "                continue\n",
    "            print(dali_img.dtype, nvjpeg_img.dtype, dicomsdl_img.dtype, pydicom_img.dtype,\n",
    "                  np.sum(dali_img - nvjpeg_img),\n",
    "                  np.sum(nvjpeg_img - dicomsdl_img),\n",
    "                  np.sum(dicomsdl_img - pydicom_img),\n",
    "                  np.max(dicomsdl_img - pydicom_img),\n",
    "                  np.mean(dicomsdl_img - pydicom_img))\n",
    "            # np.testing.assert_allclose(dali_img, dicomsdl_img)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def compare_dali_nvjpeg2k(self, save_backend='cv2', save_dtype='uint8'):\n",
    "        if save_backend == 'cv2':\n",
    "            file_ext = 'png'\n",
    "        elif save_backend == 'np':\n",
    "            file_ext = 'npy'\n",
    "    \n",
    "#         dcm_paths = self.dali_dcm_paths[:500]\n",
    "#         save_paths = self.dali_save_paths[:500]\n",
    "        \n",
    "        dcm_paths = self.dicomsdl_dcm_paths[:500]\n",
    "        save_paths = self.dicomsdl_save_paths[:500]\n",
    "\n",
    "        dali_dcm_paths = dcm_paths\n",
    "        dicomsdl_dcm_paths = dcm_paths\n",
    "        dali_save_paths = [\n",
    "            f'{p}_{save_dtype}_dali.{file_ext}' for p in save_paths\n",
    "        ]\n",
    "        nvjpeg2k_save_paths = [\n",
    "            f'{p}_{save_dtype}_nvjpeg2k.{file_ext}' for p in save_paths\n",
    "        ]\n",
    "        \n",
    "        save_root_dir = self.save_root_dir\n",
    "        \n",
    "#         # process with dali\n",
    "#         print('Convert with DALI:', len(dali_dcm_paths))\n",
    "#         start = time.time()\n",
    "#         j2k_temp_dir = os.path.join(save_root_dir, 'temp')\n",
    "#         convert_with_dali_parallel(\n",
    "#             dali_dcm_paths,\n",
    "#             dali_save_paths,\n",
    "#             save_backend = save_backend,\n",
    "#             save_dtype = save_dtype,\n",
    "#             chunk=64,  # disk\n",
    "#             batch_size=1,  # disk, ram_v3\n",
    "#             num_threads=1, # default 2\n",
    "#             py_num_workers=1,  # ram_v3\n",
    "#             device_id=0,\n",
    "#             cache='ram_v3',\n",
    "#             j2k_temp_dir=j2k_temp_dir,  # disk\n",
    "#             parallel_n_jobs=2,\n",
    "#             parallel_backend='loky')\n",
    "#         end = time.time()\n",
    "#         print(f'\\n---DALI done in {end - start} sec.\\n')\n",
    "\n",
    "        \n",
    "        \n",
    "        ############ PROCESS WITH NVJPEG2K\n",
    "        nvjpeg2k_dcm_paths = dcm_paths\n",
    "        print('Convert with nvjpeg2k:', len(nvjpeg2k_dcm_paths))\n",
    "        start = time.time()\n",
    "        convert_with_nvjpeg2k_parallel(nvjpeg2k_dcm_paths,\n",
    "                                       nvjpeg2k_save_paths,\n",
    "                                       save_backend = save_backend,\n",
    "                                       save_dtype = save_dtype,\n",
    "                                       parallel_n_jobs=4,\n",
    "                                       parallel_backend='loky')\n",
    "        end = time.time()\n",
    "        print(f'\\n---NVJPEG2K done in {end - start} sec.\\n')\n",
    "        \n",
    "\n",
    "        print('Compare results:')\n",
    "        for dali_save_path, nvjpeg2k_save_path in zip(\n",
    "                dali_save_paths, nvjpeg2k_save_paths):\n",
    "            print(dali_save_path, nvjpeg2k_save_path)\n",
    "            dali_img = load_img_from_file(dali_save_path, backend=save_backend)\n",
    "            nvjpeg2k_img = load_img_from_file(nvjpeg2k_save_path,\n",
    "                                              backend=save_backend)\n",
    "            if dali_img is None or nvjpeg2k_img is None:\n",
    "                print('???')\n",
    "                continue\n",
    "            \n",
    "            diff = np.sum(dali_img - nvjpeg2k_img)\n",
    "            print(dali_img.dtype, nvjpeg2k_img.dtype,\n",
    "                  diff)\n",
    "            assert diff == 0\n",
    "            # np.testing.assert_allclose(dali_img, nvjpeg2k_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RuntimeError: Error when starting Python worker threads for DALI parallel External Source. Cannot fork a process when the CUDA has been initialized in the process. CUDA is initialized during ``Pipeline.build()``, or can be initialized by another library that interacts with CUDA, for example a DL framework creating CUDA tensors. If you are trying to build multiple pipelines that use Python workers, you will need to call ``start_py_workers`` method on all of them before calling ``build`` method of any pipeline to start Python workers before CUDA is initialized by ``build`` or other CUDA operation. Alternatively you can change Python workers starting method from ``fork`` to ``spawn`` (see DALI Pipeline's ``py_start_method`` option for details)\n",
    "\n",
    "\n",
    "AssertionError: daemonic processes are not allowed to have children\n",
    "\n",
    "\n",
    "\n",
    "500 samples:\n",
    "- dali + job 1 + batch 1 + thread 1 + py 1: 166.7\n",
    "- dali + job 2 + batch 1 + thread 1 + py 1: 128.7, 131.4\n",
    "- dali + job 2 + batch 1 + thread 1 + py 2: 135.5\n",
    "- dali + job 2 + batch 1 + thread 1 + py 3: 137.7\n",
    "--> kaggle not bottleneck at IO, but CPU/GPU instead\n",
    "- dali + job 2 + batch 1 + thread 2 + py 1: 137.7\n",
    "- dali + job 2 + batch 1 + thread 4 + py 1: 133.9, 132.7\n",
    "- dali + job 2 + batch 1 + thread 8 + py 1: 131.8\n",
    "--> num thread not affect so much, maybe increase number of threads reduce some small amount of time\n",
    "- dali + job 2 + batch 4 + thread 1 + py 1: 133.0\n",
    "- dali + job 2 + batch 8 + thread 1 + py 1: 137.7, 135\n",
    "- dali + job 2 + batch 8 + thread 1 + py 4: 140\n",
    "--> batch size should be 1\n",
    "- dali + job 2 + batch 1 + thread 1 + py 1: 128.7, 131.4\n",
    "- dali + job 3 + batch 1 + thread 1 + py 1: 132.5\n",
    "- dali + job 4 + batch 1 + thread 1 + py 1: 132.0\n",
    "- dali + job 8 + batch 1 + thread 1 + py 1: 168.1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- nv + job 1: 216.5\n",
    "- nv + job 2: 143.1\n",
    "- nv + job 3: 139.7 \n",
    "- nv + job 4: 133.37, 134.1\n",
    "- nv + job 5: 144.7\n",
    "- nv + job 6: 174.9\n",
    "\n",
    "- dalinv + job 2: 151.5\n",
    "- dalinv + job 4: 140.8\n",
    "\n",
    "\n",
    "**LOSSLESS**:\n",
    "\n",
    "- dali + job 2 + batch 1 + thread 1 + py 1: 151.4\n",
    "- dalinv + job 4: 169.2\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "- dali + job 4 + 111: 143.8\n",
    "- dalinv + job 4: 175.3\n",
    "- dicomsdl + job 4: 166.5\n",
    "- pydicom + job 4: 221.1\n",
    "\n",
    "---\n",
    "**uint8**\n",
    "- dali + job 4 + 111: 125.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T18:58:29.902836Z",
     "iopub.status.busy": "2023-02-12T18:58:29.902086Z",
     "iopub.status.idle": "2023-02-12T19:00:11.283134Z",
     "shell.execute_reply": "2023-02-12T19:00:11.281706Z",
     "shell.execute_reply.started": "2023-02-12T18:58:29.902796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "Convert with dicomsdl: 500\n",
      "No parralel. Starting the tasks within current process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 86.33, np_v2 = 57.35, np_v3 = 37.37, torch = [6.72 0.16 24.18 31.06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 80.96, np_v2 = 49.91, np_v3 = 39.16, torch = [6.97 0.15 24.39 31.51]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:01,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 83.2, np_v2 = 53.26, np_v3 = 40.41, torch = [8.17 0.14 23.08 31.39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:02,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 83.23, np_v2 = 52.05, np_v3 = 38.02, torch = [9.8 0.14 23.12 33.06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:03,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 78.79, np_v2 = 51.53, np_v3 = 41.93, torch = [7.57 0.13 22.32 30.01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:04,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 121.87, np_v2 = 70.86, np_v3 = 61.65, torch = [11.28 0.12 26.74 38.15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [00:05,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 84.08, np_v2 = 51.05, np_v3 = 41.62, torch = [7.67 0.12 20.07 27.86]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [00:06,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 57.06, np_v2 = 28.9, np_v3 = 25.96, torch = [5.7 0.11 17.81 23.62]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [00:06,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 78.86, np_v2 = 49.42, np_v3 = 41.07, torch = [8.99 0.14 21.74 30.88]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [00:07,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 116.18, np_v2 = 61.63, np_v3 = 37.87, torch = [7.17 0.15 22.0 29.31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [00:08,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 82.64, np_v2 = 57.62, np_v3 = 45.21, torch = [7.88 0.14 21.24 29.26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [00:09,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 95.75, np_v2 = 58.61, np_v3 = 37.68, torch = [7.46 0.13 24.29 31.89]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:10,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 50.11, np_v2 = 27.51, np_v3 = 24.72, torch = [7.13 0.13 14.64 21.9]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 107.06, np_v2 = 60.76, np_v3 = 50.95, torch = [11.52 0.13 27.6 39.24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [00:11,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 95.51, np_v2 = 58.94, np_v3 = 44.45, torch = [6.85 0.14 21.05 28.04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:12,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "SIGMOID 0.0 3.0517578125e-05 3.0517578125e-05 with time np_v1 = 47.84, np_v2 = 43.04, np_v3 = 38.13, torch = [4.63 0.22 4.94 9.8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "17it [00:13,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 52.88, np_v2 = 25.18, np_v3 = 22.77, torch = [4.78 0.14 12.74 17.66]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 80.08, np_v2 = 50.18, np_v3 = 38.57, torch = [8.4 0.13 21.85 30.38]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "18it [00:14,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 82.16, np_v2 = 58.02, np_v3 = 39.29, torch = [8.49 0.13 23.26 31.88]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:15,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 67.23, np_v2 = 31.09, np_v3 = 24.1, torch = [5.1 0.13 16.5 21.73]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 77.23, np_v2 = 51.06, np_v3 = 40.45, torch = [6.89 0.11 27.71 34.71]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "21it [00:16,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 90.14, np_v2 = 52.96, np_v3 = 38.17, torch = [6.74 0.13 20.75 27.62]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "22it [00:17,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 82.6, np_v2 = 49.63, np_v3 = 38.76, torch = [6.68 0.13 24.33 31.15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "23it [00:17,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 80.18, np_v2 = 50.75, np_v3 = 42.36, torch = [7.48 0.12 24.77 32.37]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [00:18,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 84.64, np_v2 = 49.9, np_v3 = 36.68, torch = [8.96 0.12 22.14 31.22]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:19,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "SIGMOID 0.0 3.0517578125e-05 3.0517578125e-05 with time np_v1 = 24.86, np_v2 = 24.07, np_v3 = 21.77, torch = [2.53 0.22 2.64 5.39]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 82.79, np_v2 = 50.15, np_v3 = 41.3, torch = [9.6 0.13 24.69 34.42]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [00:20,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 54.48, np_v2 = 30.36, np_v3 = 24.52, torch = [5.03 0.14 12.45 17.62]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "29it [00:21,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 48.45, np_v2 = 30.68, np_v3 = 23.5, torch = [4.66 0.12 12.58 17.36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "30it [00:21,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 54.84, np_v2 = 28.54, np_v3 = 25.77, torch = [6.68 0.14 17.06 23.87]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "31it [00:22,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "SIGMOID 0.0 3.0517578125e-05 3.0517578125e-05 with time np_v1 = 43.75, np_v2 = 39.12, np_v3 = 35.31, torch = [6.77 0.23 4.76 11.75]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 99.42, np_v2 = 58.06, np_v3 = 39.03, torch = [8.91 0.16 32.8 41.87]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:23,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 60.52, np_v2 = 32.7, np_v3 = 30.38, torch = [4.51 0.12 15.17 19.8]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 94.9, np_v2 = 52.36, np_v3 = 40.04, torch = [6.66 0.13 21.34 28.12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "34it [00:24,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 84.03, np_v2 = 51.9, np_v3 = 41.69, torch = [7.25 0.13 23.46 30.84]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:26,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 48.13, np_v2 = 24.13, np_v3 = 22.83, torch = [5.23 0.12 13.7 19.05]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 75.36, np_v2 = 49.13, np_v3 = 35.92, torch = [6.65 0.13 22.51 29.3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:27,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 48.04, np_v2 = 24.19, np_v3 = 22.33, torch = [4.16 0.11 14.64 18.91]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "39it [00:28,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 47.69, np_v2 = 23.82, np_v3 = 23.1, torch = [4.25 0.12 15.12 19.48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "40it [00:28,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 48.93, np_v2 = 24.97, np_v3 = 22.88, torch = [4.29 0.13 13.17 17.58]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 91.93, np_v2 = 47.08, np_v3 = 35.87, torch = [6.43 0.16 20.29 26.89]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "41it [00:29,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 74.27, np_v2 = 46.69, np_v3 = 35.24, torch = [6.57 0.11 22.25 28.94]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "42it [00:30,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 73.95, np_v2 = 47.39, np_v3 = 36.3, torch = [7.6 0.12 23.38 31.09]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "43it [00:31,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 80.92, np_v2 = 48.77, np_v3 = 40.06, torch = [9.38 0.18 24.57 34.13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [00:32,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "SIGMOID 0.0 1.52587890625e-05 3.0517578125e-05 with time np_v1 = 24.87, np_v2 = 22.98, np_v3 = 20.74, torch = [2.54 0.24 2.18 4.96]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "46it [00:33,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "SIGMOID 0.0 3.0517578125e-05 3.0517578125e-05 with time np_v1 = 45.86, np_v2 = 39.11, np_v3 = 38.35, torch = [4.31 0.19 5.24 9.75]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 88.43, np_v2 = 59.47, np_v3 = 50.07, torch = [9.37 0.26 30.69 40.32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "47it [00:34,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 172.13, np_v2 = 122.79, np_v3 = 82.05, torch = [18.35 0.15 46.67 65.17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [00:36,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "SIGMOID 0.0 3.0517578125e-05 3.0517578125e-05 with time np_v1 = 23.78, np_v2 = 26.16, np_v3 = 20.49, torch = [2.46 0.21 2.04 4.71]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 83.64, np_v2 = 49.52, np_v3 = 40.43, torch = [6.3 0.12 24.0 30.42]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:37,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 51.12, np_v2 = 29.39, np_v3 = 26.5, torch = [5.14 0.14 17.66 22.94]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 81.33, np_v2 = 54.09, np_v3 = 45.39, torch = [8.36 0.13 21.76 30.25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53it [00:39,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 59.41, np_v2 = 25.95, np_v3 = 22.82, torch = [5.72 0.15 14.93 20.8]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 75.5, np_v2 = 48.82, np_v3 = 35.43, torch = [6.95 0.12 22.46 29.54]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:40,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 51.96, np_v2 = 27.81, np_v3 = 23.78, torch = [4.59 0.12 13.6 18.32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "56it [00:40,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 50.68, np_v2 = 28.97, np_v3 = 24.24, torch = [6.61 0.12 14.31 21.04]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 96.14, np_v2 = 55.92, np_v3 = 38.61, torch = [7.14 0.15 19.35 26.63]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [00:42,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "SIGMOID 0.0 3.0517578125e-05 3.0517578125e-05 with time np_v1 = 46.57, np_v2 = 43.74, np_v3 = 38.19, torch = [7.24 0.28 4.72 12.23]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 88.9, np_v2 = 50.59, np_v3 = 39.75, torch = [8.07 0.15 21.57 29.79]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "59it [00:43,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 78.91, np_v2 = 50.12, np_v3 = 36.85, torch = [7.02 0.14 20.69 27.85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:44,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 64.72, np_v2 = 31.76, np_v3 = 26.4, torch = [6.12 0.11 16.3 22.53]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 90.96, np_v2 = 48.87, np_v3 = 37.88, torch = [7.18 0.14 20.01 27.32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:45,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 52.29, np_v2 = 26.03, np_v3 = 23.02, torch = [6.22 0.12 15.72 22.05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "64it [00:46,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 47.24, np_v2 = 24.66, np_v3 = 22.97, torch = [4.7 0.12 15.42 20.25]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 90.63, np_v2 = 54.47, np_v3 = 40.79, torch = [6.38 0.12 21.41 27.9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:47,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "SIGMOID 0.0 1.52587890625e-05 3.0517578125e-05 with time np_v1 = 27.78, np_v2 = 23.86, np_v3 = 20.81, torch = [3.4 0.22 3.22 6.84]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 169.0, np_v2 = 102.98, np_v3 = 75.76, torch = [19.01 0.13 45.65 64.79]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "67it [00:49,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 81.52, np_v2 = 54.76, np_v3 = 36.49, torch = [6.69 0.14 20.95 27.79]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "68it [00:50,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 78.61, np_v2 = 54.51, np_v3 = 38.01, torch = [6.94 0.13 23.35 30.43]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "69it [00:50,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 163.83, np_v2 = 98.44, np_v3 = 71.85, torch = [15.67 0.15 45.04 60.86]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "70it [00:52,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 78.91, np_v2 = 51.47, np_v3 = 38.31, torch = [6.76 0.14 20.05 26.95]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "71it [00:53,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 74.75, np_v2 = 46.33, np_v3 = 35.8, torch = [6.82 0.13 21.9 28.85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "72it [00:54,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 83.63, np_v2 = 51.64, np_v3 = 36.77, torch = [6.43 0.13 24.8 31.36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:55,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "SIGMOID 0.0 3.0517578125e-05 3.0517578125e-05 with time np_v1 = 22.19, np_v2 = 27.35, np_v3 = 20.67, torch = [2.89 0.21 2.11 5.21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "75it [00:55,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 54.45, np_v2 = 26.43, np_v3 = 22.57, torch = [4.62 0.13 14.3 19.05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "76it [00:56,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 54.19, np_v2 = 27.77, np_v3 = 24.46, torch = [4.96 0.18 15.45 20.59]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "77it [00:57,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 62.26, np_v2 = 24.79, np_v3 = 27.45, torch = [5.57 0.11 17.4 23.09]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "78it [00:57,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 47.8, np_v2 = 23.94, np_v3 = 22.29, torch = [7.44 0.14 15.86 23.45]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 86.09, np_v2 = 50.77, np_v3 = 37.29, torch = [8.36 0.12 20.36 28.85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:58,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "SIGMOID 0.0 3.0517578125e-05 3.0517578125e-05 with time np_v1 = 19.89, np_v2 = 22.63, np_v3 = 28.26, torch = [2.97 0.18 2.29 5.44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "81it [00:59,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 80.22, np_v2 = 56.93, np_v3 = 37.97, torch = [6.65 0.13 33.8 40.58]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 80.18, np_v2 = 50.98, np_v3 = 37.6, torch = [7.54 0.12 22.68 30.33]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "82it [01:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 91.9, np_v2 = 55.12, np_v3 = 49.17, torch = [10.06 0.18 32.25 42.49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [01:01,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 52.98, np_v2 = 27.97, np_v3 = 23.16, torch = [7.2 0.12 17.17 24.49]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 78.6, np_v2 = 53.72, np_v3 = 36.61, torch = [6.64 0.12 19.98 26.73]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [01:02,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 70.15, np_v2 = 35.34, np_v3 = 25.59, torch = [5.01 0.12 16.02 21.16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "87it [01:03,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 54.89, np_v2 = 27.53, np_v3 = 25.37, torch = [6.22 0.13 14.71 21.06]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 79.89, np_v2 = 65.16, np_v3 = 47.99, torch = [7.82 0.12 21.37 29.31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "88it [01:04,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 89.24, np_v2 = 53.73, np_v3 = 37.89, torch = [8.39 0.12 22.42 30.94]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [01:06,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 52.48, np_v2 = 26.72, np_v3 = 23.46, torch = [4.96 0.12 13.55 18.63]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "91it [01:06,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 49.6, np_v2 = 31.0, np_v3 = 23.12, torch = [6.24 0.12 14.19 20.55]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "92it [01:07,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 52.06, np_v2 = 27.51, np_v3 = 22.63, torch = [4.51 0.12 14.16 18.78]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 85.59, np_v2 = 51.17, np_v3 = 39.07, torch = [6.91 0.13 21.58 28.61]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94it [01:08,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 52.45, np_v2 = 27.66, np_v3 = 23.84, torch = [4.54 0.12 13.14 17.8]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 80.29, np_v2 = 62.2, np_v3 = 48.07, torch = [7.2 0.13 21.74 29.08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "95it [01:09,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 91.76, np_v2 = 51.03, np_v3 = 40.72, torch = [7.09 0.11 24.19 31.39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97it [01:10,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "SIGMOID 0.0 3.0517578125e-05 3.0517578125e-05 with time np_v1 = 20.22, np_v2 = 22.36, np_v3 = 23.93, torch = [3.38 0.21 4.33 7.93]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 98.05, np_v2 = 49.74, np_v3 = 38.63, torch = [10.12 0.15 23.47 33.75]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "98it [01:11,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 86.25, np_v2 = 47.9, np_v3 = 49.56, torch = [8.51 0.12 24.57 33.2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "99it [01:11,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 85.38, np_v2 = 51.45, np_v3 = 39.66, torch = [6.55 0.13 20.41 27.08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100it [01:12,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 116.46, np_v2 = 62.55, np_v3 = 54.27, torch = [11.72 0.12 28.48 40.32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "101it [01:13,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 79.38, np_v2 = 48.79, np_v3 = 39.28, torch = [8.82 0.12 28.64 37.59]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [01:15,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 56.74, np_v2 = 26.12, np_v3 = 25.41, torch = [5.9 0.14 14.91 20.95]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 48.23, np_v2 = 26.59, np_v3 = 24.19, torch = [6.1 0.12 15.56 21.78]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "104it [01:15,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 55.28, np_v2 = 28.28, np_v3 = 26.83, torch = [6.88 0.12 15.93 22.93]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "105it [01:16,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 76.43, np_v2 = 48.97, np_v3 = 37.08, torch = [6.88 0.11 20.69 27.68]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "107it [01:17,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 53.76, np_v2 = 29.1, np_v3 = 25.09, torch = [4.37 0.12 13.25 17.73]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 82.54, np_v2 = 53.53, np_v3 = 38.8, torch = [7.41 0.13 21.25 28.8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "108it [01:18,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 86.04, np_v2 = 55.02, np_v3 = 49.01, torch = [7.02 0.12 20.21 27.35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [01:19,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "SIGMOID 0.0 3.0517578125e-05 3.0517578125e-05 with time np_v1 = 24.18, np_v2 = 21.74, np_v3 = 21.39, torch = [2.72 0.2 2.11 5.03]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 102.51, np_v2 = 64.02, np_v3 = 38.3, torch = [7.56 0.13 22.07 29.76]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "111it [01:20,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 95.61, np_v2 = 83.13, np_v3 = 61.01, torch = [8.32 0.12 35.72 44.16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "112it [01:21,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 148.16, np_v2 = 94.99, np_v3 = 70.27, torch = [7.9 0.13 27.07 35.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "113it [01:22,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 175.49, np_v2 = 93.72, np_v3 = 50.84, torch = [7.32 0.12 21.2 28.64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "114it [01:23,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 114.02, np_v2 = 61.54, np_v3 = 47.03, torch = [10.02 0.12 21.47 31.62]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "116it [01:25,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 62.33, np_v2 = 28.27, np_v3 = 34.29, torch = [5.9 0.12 14.7 20.73]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "117it [01:25,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "SIGMOID 0.0 3.0517578125e-05 3.0517578125e-05 with time np_v1 = 27.44, np_v2 = 22.7, np_v3 = 21.5, torch = [4.71 0.2 4.12 9.03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "118it [01:26,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 62.52, np_v2 = 36.32, np_v3 = 25.62, torch = [4.81 0.12 14.6 19.54]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "119it [01:26,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "SIGMOID 0.0 3.0517578125e-05 3.0517578125e-05 with time np_v1 = 43.34, np_v2 = 38.88, np_v3 = 35.21, torch = [5.05 0.17 4.09 9.3]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 85.52, np_v2 = 55.79, np_v3 = 37.36, torch = [7.32 0.12 21.49 28.92]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "120it [01:27,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 141.43, np_v2 = 109.42, np_v3 = 101.15, torch = [9.84 0.14 21.74 31.72]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "121it [01:28,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 88.01, np_v2 = 55.63, np_v3 = 42.63, torch = [7.55 0.15 26.4 34.09]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [01:30,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 52.43, np_v2 = 30.98, np_v3 = 33.14, torch = [5.06 0.14 15.3 20.51]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "124it [01:31,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 81.52, np_v2 = 58.39, np_v3 = 40.64, torch = [6.87 0.13 23.0 30.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "125it [01:31,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 48.96, np_v2 = 25.29, np_v3 = 23.88, torch = [6.18 0.13 17.06 23.37]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 99.67, np_v2 = 53.78, np_v3 = 38.41, torch = [8.3 0.12 20.28 28.7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [01:32,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 50.67, np_v2 = 28.79, np_v3 = 25.57, torch = [4.93 0.13 13.8 18.87]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 91.41, np_v2 = 63.0, np_v3 = 44.37, torch = [7.78 0.13 23.3 31.21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "128it [01:33,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 82.09, np_v2 = 51.29, np_v3 = 42.26, torch = [6.71 0.13 22.79 29.63]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "129it [01:34,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 89.69, np_v2 = 55.95, np_v3 = 43.79, torch = [7.34 0.13 22.52 29.99]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "130it [01:35,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 94.64, np_v2 = 64.61, np_v3 = 45.18, torch = [8.63 0.12 21.49 30.24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "131it [01:36,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 211.77, np_v2 = 123.73, np_v3 = 85.52, torch = [18.68 0.13 44.06 62.87]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "133it [01:38,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 53.41, np_v2 = 29.6, np_v3 = 34.16, torch = [7.77 0.16 14.58 22.51]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "134it [01:39,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 58.02, np_v2 = 36.21, np_v3 = 29.04, torch = [5.13 0.11 16.15 21.39]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 96.81, np_v2 = 57.08, np_v3 = 39.02, torch = [12.47 0.17 24.23 36.87]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "135it [01:40,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 83.87, np_v2 = 56.13, np_v3 = 38.75, torch = [7.39 0.12 20.38 27.89]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "136it [01:40,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 94.4, np_v2 = 53.58, np_v3 = 38.39, torch = [6.93 0.11 23.53 30.57]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "138it [01:42,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 56.92, np_v2 = 33.55, np_v3 = 27.53, torch = [6.41 0.12 14.37 20.91]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "139it [01:42,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 59.29, np_v2 = 28.96, np_v3 = 23.57, torch = [5.65 0.13 12.86 18.64]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 51.41, np_v2 = 26.41, np_v3 = 24.48, torch = [5.67 0.12 12.71 18.51]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "140it [01:43,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 84.88, np_v2 = 52.32, np_v3 = 43.09, torch = [6.59 0.12 22.17 28.88]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "142it [01:44,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "SIGMOID 0.0 3.0517578125e-05 3.0517578125e-05 with time np_v1 = 24.68, np_v2 = 25.76, np_v3 = 23.52, torch = [3.55 0.21 3.93 7.69]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 104.47, np_v2 = 58.18, np_v3 = 43.98, torch = [8.38 0.12 23.45 31.95]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "143it [01:45,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 55.64, np_v2 = 27.59, np_v3 = 26.05, torch = [4.87 0.12 14.2 19.19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "144it [01:45,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 54.12, np_v2 = 25.08, np_v3 = 23.38, torch = [4.68 0.12 13.78 18.58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146it [01:46,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "SIGMOID 0.0 3.0517578125e-05 3.0517578125e-05 with time np_v1 = 22.99, np_v2 = 23.06, np_v3 = 20.27, torch = [2.53 0.17 2.24 4.94]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "147it [01:47,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 60.13, np_v2 = 27.32, np_v3 = 25.06, torch = [5.3 0.12 12.74 18.16]\n",
      "??? tensor(0, device='cuda:0', dtype=torch.int16)\n",
      "LINEAR 0.0 1.52587890625e-05 1.52587890625e-05 with time np_v1 = 181.01, np_v2 = 107.73, np_v3 = 85.94, torch = [17.81 0.13 43.99 61.92]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "147it [01:48,  1.35it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m tester \u001b[38;5;241m=\u001b[39m dicom\u001b[38;5;241m.\u001b[39m_Tester()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# tester.speed_compare()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# tester.dali_parallel()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# tester.dicomsdl_parallel()\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompare_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcv2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muint8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/dangnh36/projects/.comp/rsna/src/submit/dicom.py:1445\u001b[0m, in \u001b[0;36m_Tester.compare_results\u001b[0;34m(self, save_backend, save_dtype)\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvert with dicomsdl:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dicomsdl_dcm_paths))\n\u001b[1;32m   1444\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1445\u001b[0m \u001b[43mconvert_with_dicomsdl_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdicomsdl_dcm_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdicomsdl_save_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m                               \u001b[49m\u001b[43msave_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msave_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m                               \u001b[49m\u001b[43msave_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msave_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mparallel_n_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mparallel_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloky\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1451\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---Dicomsdl done in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/home/dangnh36/projects/.comp/rsna/src/submit/dicom.py:1046\u001b[0m, in \u001b[0;36mconvert_with_dicomsdl_parallel\u001b[0;34m(dcm_paths, save_paths, save_backend, save_dtype, parallel_n_jobs, parallel_backend)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel_n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo parralel. Starting the tasks within current process.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_with_dicomsdl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdcm_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43msave_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparallel_n_jobs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m jobs with backend `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparallel_backend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`...\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1051\u001b[0m     )\n",
      "File \u001b[0;32m/home/dangnh36/projects/.comp/rsna/src/submit/dicom.py:1033\u001b[0m, in \u001b[0;36mconvert_with_dicomsdl\u001b[0;34m(dcm_paths, save_paths, save_backend, save_dtype)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dcm_paths) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(save_paths)\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dcm_path, save_path \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mzip\u001b[39m(dcm_paths, save_paths)):\n\u001b[0;32m-> 1033\u001b[0m     \u001b[43m_convert_single_with_dicomsdl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdcm_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43msave_dtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/dangnh36/projects/.comp/rsna/src/submit/dicom.py:1024\u001b[0m, in \u001b[0;36m_convert_single_with_dicomsdl\u001b[0;34m(dcm_path, save_path, save_backend, save_dtype)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_single_with_dicomsdl\u001b[39m(dcm_path,\n\u001b[1;32m   1020\u001b[0m                                   save_path,\n\u001b[1;32m   1021\u001b[0m                                   save_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1022\u001b[0m                                   save_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1023\u001b[0m     img \u001b[38;5;241m=\u001b[39m load_img_dicomsdl(dcm_path, dtype\u001b[38;5;241m=\u001b[39msave_dtype)\n\u001b[0;32m-> 1024\u001b[0m     \u001b[43msave_img_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_backend\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/dangnh36/projects/.comp/rsna/src/submit/dicom.py:302\u001b[0m, in \u001b[0;36msave_img_to_file\u001b[0;34m(save_path, img, backend)\u001b[0m\n\u001b[1;32m    300\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimwrite(save_path, img)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39muint8:\n\u001b[0;32m--> 302\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`cv2` backend only support uint8 or uint16 images.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import dicom\n",
    "importlib.reload(dicom)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tester = dicom._Tester()\n",
    "    # tester.speed_compare()\n",
    "    # tester.dali_parallel()\n",
    "    # tester.dicomsdl_parallel()\n",
    "    tester.compare_results(save_backend='cv2', save_dtype='uint8')\n",
    "#     tester.compare_dali_nvjpeg2k(save_backend='cv2', save_dtype='uint16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Megvii-BaseDetection/YOLOX.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd YOLOX\n",
    "!pip3 install -v -e .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd YOLOX\n",
    "!git clone https://github.com/NVIDIA-AI-IOT/torch2trt\n",
    "!cd torch2trt\n",
    "!python setup.py install\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd YOLOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/kaggle/working/YOLOX/')\n",
    "sys.path.append('/kaggle/working/YOLOX/torch2trt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTHONPATH=$(pwd):$(pwd)/torch2trt python3 tools/trt.py -f /kaggle/input/kaggle-rsna/yolox_nano_bre_416.py -c /kaggle/input/kaggle-rsna/yolox_nano_bre_416_v2.pth -b 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /kaggle/working/yolox_nano_bre_416/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv /kaggle/working/yolox_nano_bre_416/* /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
