{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443ebcd5",
   "metadata": {
    "papermill": {
     "duration": 0.008398,
     "end_time": "2022-12-14T10:07:18.990860",
     "exception": false,
     "start_time": "2022-12-14T10:07:18.982462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# RSNA Breast Baseline - Faster Inference with NVIDIA Dali\n",
    "\n",
    "This notebook shows a simple PyTorch inference pipeline based on the pregenerated datasets.\n",
    "Dicom preprocessing is accelerated on GPU !\n",
    "\n",
    "*Overall runtime is ~3h30h !*\n",
    "\n",
    "**GPU Dicom acceleration :**\n",
    "All credits go to @tivfrvqhs5\n",
    "- https://www.kaggle.com/competitions/rsna-breast-cancer-detection/discussion/371534\n",
    "- https://www.kaggle.com/code/tivfrvqhs5/decode-jpeg2000-dicom-with-dali\n",
    "\n",
    "**Using dicomsdl :**\n",
    "- https://www.kaggle.com/code/hengck23/combine-dali-and-dicomsdl-for-reading-dicom-files\n",
    "\n",
    "**Training Dataset :**\n",
    "\n",
    "- LB 0.46 -> https://www.kaggle.com/datasets/theoviel/rsna-breast-cancer-1024-pngs\n",
    "- LB 0.24 -> https://www.kaggle.com/datasets/theoviel/rsna-breast-cancer-512-pngs\n",
    "\n",
    "*plz upvote ^*\n",
    "\n",
    "**Changes :**\n",
    "- **Forked from** : https://www.kaggle.com/code/theoviel/rsna-breast-baseline-inference\n",
    "- **v4** : Threshold after averaging, GPU acceleration, img_size=1024\n",
    "- **v7** : Add dicomsdl for more speed-up !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae16d5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T10:07:19.006777Z",
     "iopub.status.busy": "2022-12-14T10:07:19.006010Z",
     "iopub.status.idle": "2022-12-14T10:07:19.016658Z",
     "shell.execute_reply": "2022-12-14T10:07:19.015802Z"
    },
    "papermill": {
     "duration": 0.020818,
     "end_time": "2022-12-14T10:07:19.018691",
     "exception": false,
     "start_time": "2022-12-14T10:07:18.997873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44008145",
   "metadata": {
    "papermill": {
     "duration": 0.00676,
     "end_time": "2022-12-14T10:07:19.032244",
     "exception": false,
     "start_time": "2022-12-14T10:07:19.025484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialization\n",
    "- Install Dali + Overwrite a file to handle UINT16\n",
    "- Install dicomsdl, pylibjpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bceea61c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-12-14T10:07:19.047618Z",
     "iopub.status.busy": "2022-12-14T10:07:19.046844Z",
     "iopub.status.idle": "2022-12-14T10:09:02.529635Z",
     "shell.execute_reply": "2022-12-14T10:09:02.528320Z"
    },
    "papermill": {
     "duration": 103.493177,
     "end_time": "2022-12-14T10:09:02.532261",
     "exception": false,
     "start_time": "2022-12-14T10:07:19.039084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/rsna-2022-whl/pydicom-2.3.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/rsna-2022-whl/pylibjpeg-1.4.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/rsna-2022-whl/python_gdcm-3.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pylibjpeg==1.4.0) (1.21.6)\r\n",
      "pydicom is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Installing collected packages: python-gdcm, pylibjpeg\r\n",
      "Successfully installed pylibjpeg-1.4.0 python-gdcm-3.0.15\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/nvidia-dali-wheel/nvidia_dali_nightly_cuda110-1.22.0.dev20221213-6757685-py3-none-manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from nvidia-dali-nightly-cuda110==1.22.0.dev20221213) (1.6.3)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from nvidia-dali-nightly-cuda110==1.22.0.dev20221213) (0.4.0)\r\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->nvidia-dali-nightly-cuda110==1.22.0.dev20221213) (1.15.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->nvidia-dali-nightly-cuda110==1.22.0.dev20221213) (0.37.1)\r\n",
      "Installing collected packages: nvidia-dali-nightly-cuda110\r\n",
      "Successfully installed nvidia-dali-nightly-cuda110-1.22.0.dev20221213\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/nvidia-dali-wheel/dicomsdl-0.109.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\r\n",
      "Installing collected packages: dicomsdl\r\n",
      "Successfully installed dicomsdl-0.109.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/rsna-2022-whl/{pydicom-2.3.0-py3-none-any.whl,pylibjpeg-1.4.0-py3-none-any.whl,python_gdcm-3.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl}\n",
    "!pip install /kaggle/input/nvidia-dali-wheel/nvidia_dali_nightly_cuda110-1.22.0.dev20221213-6757685-py3-none-manylinux2014_x86_64.whl\n",
    "!pip install /kaggle/input/nvidia-dali-wheel/dicomsdl-0.109.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c9e3011",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:02.550833Z",
     "iopub.status.busy": "2022-12-14T10:09:02.550485Z",
     "iopub.status.idle": "2022-12-14T10:09:02.570364Z",
     "shell.execute_reply": "2022-12-14T10:09:02.569197Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.033096,
     "end_time": "2022-12-14T10:09:02.573830",
     "exception": false,
     "start_time": "2022-12-14T10:09:02.540734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /opt/conda/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /opt/conda/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py\n",
    "\n",
    "# Copyright (c) 2017-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from nvidia.dali.backend import TensorGPU, TensorListGPU\n",
    "from nvidia.dali.pipeline import Pipeline\n",
    "import nvidia.dali.ops as ops\n",
    "from nvidia.dali import types\n",
    "from nvidia.dali.plugin.base_iterator import _DaliBaseIterator\n",
    "from nvidia.dali.plugin.base_iterator import LastBatchPolicy\n",
    "import torch\n",
    "import torch.utils.dlpack as torch_dlpack\n",
    "import ctypes\n",
    "import numpy as np\n",
    "\n",
    "to_torch_type = {\n",
    "    types.DALIDataType.FLOAT:   torch.float32,\n",
    "    types.DALIDataType.FLOAT64: torch.float64,\n",
    "    types.DALIDataType.FLOAT16: torch.float16,\n",
    "    types.DALIDataType.UINT8:   torch.uint8,\n",
    "    types.DALIDataType.INT8:    torch.int8,\n",
    "    types.DALIDataType.UINT16:  torch.int16,\n",
    "    types.DALIDataType.INT16:   torch.int16,\n",
    "    types.DALIDataType.INT32:   torch.int32,\n",
    "    types.DALIDataType.INT64:   torch.int64\n",
    "}\n",
    "\n",
    "\n",
    "def feed_ndarray(dali_tensor, arr, cuda_stream=None):\n",
    "    \"\"\"\n",
    "    Copy contents of DALI tensor to PyTorch's Tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    `dali_tensor` : nvidia.dali.backend.TensorCPU or nvidia.dali.backend.TensorGPU\n",
    "                    Tensor from which to copy\n",
    "    `arr` : torch.Tensor\n",
    "            Destination of the copy\n",
    "    `cuda_stream` : torch.cuda.Stream, cudaStream_t or any value that can be cast to cudaStream_t.\n",
    "                    CUDA stream to be used for the copy\n",
    "                    (if not provided, an internal user stream will be selected)\n",
    "                    In most cases, using pytorch's current stream is expected (for example,\n",
    "                    if we are copying to a tensor allocated with torch.zeros(...))\n",
    "    \"\"\"\n",
    "    dali_type = to_torch_type[dali_tensor.dtype]\n",
    "\n",
    "    assert dali_type == arr.dtype, (\"The element type of DALI Tensor/TensorList\"\n",
    "                                    \" doesn't match the element type of the target PyTorch Tensor: \"\n",
    "                                    \"{} vs {}\".format(dali_type, arr.dtype))\n",
    "    assert dali_tensor.shape() == list(arr.size()), \\\n",
    "        (\"Shapes do not match: DALI tensor has size {0}, but PyTorch Tensor has size {1}\".\n",
    "            format(dali_tensor.shape(), list(arr.size())))\n",
    "    cuda_stream = types._raw_cuda_stream(cuda_stream)\n",
    "\n",
    "    # turn raw int to a c void pointer\n",
    "    c_type_pointer = ctypes.c_void_p(arr.data_ptr())\n",
    "    if isinstance(dali_tensor, (TensorGPU, TensorListGPU)):\n",
    "        stream = None if cuda_stream is None else ctypes.c_void_p(cuda_stream)\n",
    "        dali_tensor.copy_to_external(c_type_pointer, stream, non_blocking=True)\n",
    "    else:\n",
    "        dali_tensor.copy_to_external(c_type_pointer)\n",
    "    return arr\n",
    "\n",
    "\n",
    "class DALIGenericIterator(_DaliBaseIterator):\n",
    "    \"\"\"\n",
    "    General DALI iterator for PyTorch. It can return any number of\n",
    "    outputs from the DALI pipeline in the form of PyTorch's Tensors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pipelines : list of nvidia.dali.Pipeline\n",
    "                List of pipelines to use\n",
    "    output_map : list of str\n",
    "                List of strings which maps consecutive outputs\n",
    "                of DALI pipelines to user specified name.\n",
    "                Outputs will be returned from iterator as dictionary\n",
    "                of those names.\n",
    "                Each name should be distinct\n",
    "    size : int, default = -1\n",
    "                Number of samples in the shard for the wrapped pipeline (if there is more than\n",
    "                one it is a sum)\n",
    "                Providing -1 means that the iterator will work until StopIteration is raised\n",
    "                from the inside of iter_setup(). The options `last_batch_policy` and\n",
    "                `last_batch_padded` don't work in such case. It works with only one pipeline inside\n",
    "                the iterator.\n",
    "                Mutually exclusive with `reader_name` argument\n",
    "    reader_name : str, default = None\n",
    "                Name of the reader which will be queried to the shard size, number of shards and\n",
    "                all other properties necessary to count properly the number of relevant and padded\n",
    "                samples that iterator needs to deal with. It automatically sets `last_batch_policy`\n",
    "                to PARTIAL when the FILL is used, and `last_batch_padded` accordingly to match\n",
    "                the reader's configuration\n",
    "    auto_reset : string or bool, optional, default = False\n",
    "                Whether the iterator resets itself for the next epoch or it requires reset() to be\n",
    "                called explicitly.\n",
    "\n",
    "                It can be one of the following values:\n",
    "\n",
    "                * ``\"no\"``, ``False`` or ``None`` - at the end of epoch StopIteration is raised\n",
    "                  and reset() needs to be called\n",
    "                * ``\"yes\"`` or ``\"True\"``- at the end of epoch StopIteration is raised but reset()\n",
    "                  is called internally automatically\n",
    "\n",
    "    dynamic_shape : any, optional,\n",
    "                Parameter used only for backward compatibility.\n",
    "    fill_last_batch : bool, optional, default = None\n",
    "                **Deprecated** Please use ``last_batch_policy`` instead\n",
    "\n",
    "                Whether to fill the last batch with data up to 'self.batch_size'.\n",
    "                The iterator would return the first integer multiple\n",
    "                of self._num_gpus * self.batch_size entries which exceeds 'size'.\n",
    "                Setting this flag to False will cause the iterator to return\n",
    "                exactly 'size' entries.\n",
    "    last_batch_policy: optional, default = LastBatchPolicy.FILL\n",
    "                What to do with the last batch when there are not enough samples in the epoch\n",
    "                to fully fill it. See :meth:`nvidia.dali.plugin.base_iterator.LastBatchPolicy`\n",
    "    last_batch_padded : bool, optional, default = False\n",
    "                Whether the last batch provided by DALI is padded with the last sample\n",
    "                or it just wraps up. In the conjunction with ``last_batch_policy`` it tells\n",
    "                if the iterator returning last batch with data only partially filled with\n",
    "                data from the current epoch is dropping padding samples or samples from\n",
    "                the next epoch. If set to ``False`` next\n",
    "                epoch will end sooner as data from it was consumed but dropped. If set to\n",
    "                True next epoch would be the same length as the first one. For this to happen,\n",
    "                the option `pad_last_batch` in the reader needs to be set to True as well.\n",
    "                It is overwritten when `reader_name` argument is provided\n",
    "    prepare_first_batch : bool, optional, default = True\n",
    "                Whether DALI should buffer the first batch right after the creation of the iterator,\n",
    "                so one batch is already prepared when the iterator is prompted for the data\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    With the data set ``[1,2,3,4,5,6,7]`` and the batch size 2:\n",
    "\n",
    "    last_batch_policy = LastBatchPolicy.PARTIAL, last_batch_padded = True  -> last batch = ``[7]``,\n",
    "    next iteration will return ``[1, 2]``\n",
    "\n",
    "    last_batch_policy = LastBatchPolicy.PARTIAL, last_batch_padded = False -> last batch = ``[7]``,\n",
    "    next iteration will return ``[2, 3]``\n",
    "\n",
    "    last_batch_policy = LastBatchPolicy.FILL, last_batch_padded = True   -> last batch = ``[7, 7]``,\n",
    "    next iteration will return ``[1, 2]``\n",
    "\n",
    "    last_batch_policy = LastBatchPolicy.FILL, last_batch_padded = False  -> last batch = ``[7, 1]``,\n",
    "    next iteration will return ``[2, 3]``\n",
    "\n",
    "    last_batch_policy = LastBatchPolicy.DROP, last_batch_padded = True   -> last batch = ``[5, 6]``,\n",
    "    next iteration will return ``[1, 2]``\n",
    "\n",
    "    last_batch_policy = LastBatchPolicy.DROP, last_batch_padded = False  -> last batch = ``[5, 6]``,\n",
    "    next iteration will return ``[2, 3]``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pipelines,\n",
    "                 output_map,\n",
    "                 size=-1,\n",
    "                 reader_name=None,\n",
    "                 auto_reset=False,\n",
    "                 fill_last_batch=None,\n",
    "                 dynamic_shape=False,\n",
    "                 last_batch_padded=False,\n",
    "                 last_batch_policy=LastBatchPolicy.FILL,\n",
    "                 prepare_first_batch=True):\n",
    "\n",
    "        # check the assert first as _DaliBaseIterator would run the prefetch\n",
    "        assert len(set(output_map)) == len(output_map), \"output_map names should be distinct\"\n",
    "        self._output_categories = set(output_map)\n",
    "        self.output_map = output_map\n",
    "\n",
    "        _DaliBaseIterator.__init__(self,\n",
    "                                   pipelines,\n",
    "                                   size,\n",
    "                                   reader_name,\n",
    "                                   auto_reset,\n",
    "                                   fill_last_batch,\n",
    "                                   last_batch_padded,\n",
    "                                   last_batch_policy,\n",
    "                                   prepare_first_batch=prepare_first_batch)\n",
    "\n",
    "        self._first_batch = None\n",
    "        if self._prepare_first_batch:\n",
    "            try:\n",
    "                self._first_batch = DALIGenericIterator.__next__(self)\n",
    "                # call to `next` sets _ever_consumed to True but if we are just calling it from\n",
    "                # here we should set if to False again\n",
    "                self._ever_consumed = False\n",
    "            except StopIteration:\n",
    "                assert False, \"It seems that there is no data in the pipeline. This may happen \" \\\n",
    "                       \"if `last_batch_policy` is set to PARTIAL and the requested batch size is \" \\\n",
    "                       \"greater than the shard size.\"\n",
    "\n",
    "    def __next__(self):\n",
    "        self._ever_consumed = True\n",
    "        if self._first_batch is not None:\n",
    "            batch = self._first_batch\n",
    "            self._first_batch = None\n",
    "            return batch\n",
    "\n",
    "        # Gather outputs\n",
    "        outputs = self._get_outputs()\n",
    "\n",
    "        data_batches = [None for i in range(self._num_gpus)]\n",
    "        for i in range(self._num_gpus):\n",
    "            dev_id = self._pipes[i].device_id\n",
    "            # initialize dict for all output categories\n",
    "            category_outputs = dict()\n",
    "            # segregate outputs into categories\n",
    "            for j, out in enumerate(outputs[i]):\n",
    "                category_outputs[self.output_map[j]] = out\n",
    "\n",
    "            # Change DALI TensorLists into Tensors\n",
    "            category_tensors = dict()\n",
    "            category_shapes = dict()\n",
    "            for category, out in category_outputs.items():\n",
    "                category_tensors[category] = out.as_tensor()\n",
    "                category_shapes[category] = category_tensors[category].shape()\n",
    "\n",
    "            category_torch_type = dict()\n",
    "            category_device = dict()\n",
    "            torch_gpu_device = None\n",
    "            torch_cpu_device = torch.device('cpu')\n",
    "            # check category and device\n",
    "            for category in self._output_categories:\n",
    "                category_torch_type[category] = to_torch_type[category_tensors[category].dtype]\n",
    "                if type(category_tensors[category]) is TensorGPU:\n",
    "                    if not torch_gpu_device:\n",
    "                        torch_gpu_device = torch.device('cuda', dev_id)\n",
    "                    category_device[category] = torch_gpu_device\n",
    "                else:\n",
    "                    category_device[category] = torch_cpu_device\n",
    "\n",
    "            pyt_tensors = dict()\n",
    "            for category in self._output_categories:\n",
    "                pyt_tensors[category] = torch.empty(category_shapes[category],\n",
    "                                                    dtype=category_torch_type[category],\n",
    "                                                    device=category_device[category])\n",
    "\n",
    "            data_batches[i] = pyt_tensors\n",
    "\n",
    "            # Copy data from DALI Tensors to torch tensors\n",
    "            for category, tensor in category_tensors.items():\n",
    "                if isinstance(tensor, (TensorGPU, TensorListGPU)):\n",
    "                    # Using same cuda_stream used by torch.zeros to set the memory\n",
    "                    stream = torch.cuda.current_stream(device=pyt_tensors[category].device)\n",
    "                    feed_ndarray(tensor, pyt_tensors[category], cuda_stream=stream)\n",
    "                else:\n",
    "                    feed_ndarray(tensor, pyt_tensors[category])\n",
    "\n",
    "        self._schedule_runs()\n",
    "\n",
    "        self._advance_and_check_drop_last()\n",
    "\n",
    "        if self._reader_name:\n",
    "            if_drop, left = self._remove_padded()\n",
    "            if np.any(if_drop):\n",
    "                output = []\n",
    "                for batch, to_copy in zip(data_batches, left):\n",
    "                    batch = batch.copy()\n",
    "                    for category in self._output_categories:\n",
    "                        batch[category] = batch[category][0:to_copy]\n",
    "                    output.append(batch)\n",
    "                return output\n",
    "\n",
    "        else:\n",
    "            if self._last_batch_policy == LastBatchPolicy.PARTIAL and (\n",
    "                                          self._counter > self._size) and self._size > 0:\n",
    "                # First calculate how much data is required to return exactly self._size entries.\n",
    "                diff = self._num_gpus * self.batch_size - (self._counter - self._size)\n",
    "                # Figure out how many GPUs to grab from.\n",
    "                numGPUs_tograb = int(np.ceil(diff / self.batch_size))\n",
    "                # Figure out how many results to grab from the last GPU\n",
    "                # (as a fractional GPU batch may be required to bring us\n",
    "                # right up to self._size).\n",
    "                mod_diff = diff % self.batch_size\n",
    "                data_fromlastGPU = mod_diff if mod_diff else self.batch_size\n",
    "\n",
    "                # Grab the relevant data.\n",
    "                # 1) Grab everything from the relevant GPUs.\n",
    "                # 2) Grab the right data from the last GPU.\n",
    "                # 3) Append data together correctly and return.\n",
    "                output = data_batches[0:numGPUs_tograb]\n",
    "                output[-1] = output[-1].copy()\n",
    "                for category in self._output_categories:\n",
    "                    output[-1][category] = output[-1][category][0:data_fromlastGPU]\n",
    "                return output\n",
    "\n",
    "        return data_batches\n",
    "\n",
    "\n",
    "class DALIClassificationIterator(DALIGenericIterator):\n",
    "    \"\"\"\n",
    "    DALI iterator for classification tasks for PyTorch. It returns 2 outputs\n",
    "    (data and label) in the form of PyTorch's Tensor.\n",
    "\n",
    "    Calling\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "       DALIClassificationIterator(pipelines, reader_name)\n",
    "\n",
    "    is equivalent to calling\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "       DALIGenericIterator(pipelines, [\"data\", \"label\"], reader_name)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pipelines : list of nvidia.dali.Pipeline\n",
    "                List of pipelines to use\n",
    "    size : int, default = -1\n",
    "                Number of samples in the shard for the wrapped pipeline (if there is more than\n",
    "                one it is a sum)\n",
    "                Providing -1 means that the iterator will work until StopIteration is raised\n",
    "                from the inside of iter_setup(). The options `last_batch_policy` and\n",
    "                `last_batch_padded` don't work in such case. It works with only one pipeline inside\n",
    "                the iterator.\n",
    "                Mutually exclusive with `reader_name` argument\n",
    "    reader_name : str, default = None\n",
    "                Name of the reader which will be queried to the shard size, number of shards and\n",
    "                all other properties necessary to count properly the number of relevant and padded\n",
    "                samples that iterator needs to deal with. It automatically sets `last_batch_policy`\n",
    "                to PARTIAL when the FILL is used, and `last_batch_padded` accordingly to match\n",
    "                the reader's configuration\n",
    "    auto_reset : string or bool, optional, default = False\n",
    "                Whether the iterator resets itself for the next epoch or it requires reset() to be\n",
    "                called explicitly.\n",
    "\n",
    "                It can be one of the following values:\n",
    "\n",
    "                * ``\"no\"``, ``False`` or ``None`` - at the end of epoch StopIteration is raised\n",
    "                  and reset() needs to be called\n",
    "                * ``\"yes\"`` or ``\"True\"``- at the end of epoch StopIteration is raised but reset()\n",
    "                  is called internally automatically\n",
    "\n",
    "    dynamic_shape : any, optional,\n",
    "                Parameter used only for backward compatibility.\n",
    "    fill_last_batch : bool, optional, default = None\n",
    "                **Deprecated** Please use ``last_batch_policy`` instead\n",
    "\n",
    "                Whether to fill the last batch with data up to 'self.batch_size'.\n",
    "                The iterator would return the first integer multiple\n",
    "                of self._num_gpus * self.batch_size entries which exceeds 'size'.\n",
    "                Setting this flag to False will cause the iterator to return\n",
    "                exactly 'size' entries.\n",
    "    last_batch_policy: optional, default = LastBatchPolicy.FILL\n",
    "                What to do with the last batch when there are not enough samples in the epoch\n",
    "                to fully fill it. See :meth:`nvidia.dali.plugin.base_iterator.LastBatchPolicy`\n",
    "    last_batch_padded : bool, optional, default = False\n",
    "                Whether the last batch provided by DALI is padded with the last sample\n",
    "                or it just wraps up. In the conjunction with ``last_batch_policy`` it tells\n",
    "                if the iterator returning last batch with data only partially filled with\n",
    "                data from the current epoch is dropping padding samples or samples from\n",
    "                the next epoch. If set to ``False`` next\n",
    "                epoch will end sooner as data from it was consumed but dropped. If set to\n",
    "                True next epoch would be the same length as the first one. For this to happen,\n",
    "                the option `pad_last_batch` in the reader needs to be set to True as well.\n",
    "                It is overwritten when `reader_name` argument is provided\n",
    "    prepare_first_batch : bool, optional, default = True\n",
    "                Whether DALI should buffer the first batch right after the creation of the iterator,\n",
    "                so one batch is already prepared when the iterator is prompted for the data\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    With the data set ``[1,2,3,4,5,6,7]`` and the batch size 2:\n",
    "\n",
    "    last_batch_policy = LastBatchPolicy.PARTIAL, last_batch_padded = True  -> last batch = ``[7]``,\n",
    "    next iteration will return ``[1, 2]``\n",
    "\n",
    "    last_batch_policy = LastBatchPolicy.PARTIAL, last_batch_padded = False -> last batch = ``[7]``,\n",
    "    next iteration will return ``[2, 3]``\n",
    "\n",
    "    last_batch_policy = LastBatchPolicy.FILL, last_batch_padded = True   -> last batch = ``[7, 7]``,\n",
    "    next iteration will return ``[1, 2]``\n",
    "\n",
    "    last_batch_policy = LastBatchPolicy.FILL, last_batch_padded = False  -> last batch = ``[7, 1]``,\n",
    "    next iteration will return ``[2, 3]``\n",
    "\n",
    "    last_batch_policy = LastBatchPolicy.DROP, last_batch_padded = True   -> last batch = ``[5, 6]``,\n",
    "    next iteration will return ``[1, 2]``\n",
    "\n",
    "    last_batch_policy = LastBatchPolicy.DROP, last_batch_padded = False  -> last batch = ``[5, 6]``,\n",
    "    next iteration will return ``[2, 3]``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pipelines,\n",
    "                 size=-1,\n",
    "                 reader_name=None,\n",
    "                 auto_reset=False,\n",
    "                 fill_last_batch=None,\n",
    "                 dynamic_shape=False,\n",
    "                 last_batch_padded=False,\n",
    "                 last_batch_policy=LastBatchPolicy.FILL,\n",
    "                 prepare_first_batch=True):\n",
    "        super(DALIClassificationIterator, self).__init__(pipelines, [\"data\", \"label\"],\n",
    "                                                         size,\n",
    "                                                         reader_name=reader_name,\n",
    "                                                         auto_reset=auto_reset,\n",
    "                                                         fill_last_batch=fill_last_batch,\n",
    "                                                         dynamic_shape=dynamic_shape,\n",
    "                                                         last_batch_padded=last_batch_padded,\n",
    "                                                         last_batch_policy=last_batch_policy,\n",
    "                                                         prepare_first_batch=prepare_first_batch)\n",
    "\n",
    "\n",
    "class TorchPythonFunction(ops.PythonFunctionBase):\n",
    "    schema_name = \"TorchPythonFunction\"\n",
    "    ops.register_cpu_op('TorchPythonFunction')\n",
    "    ops.register_gpu_op('TorchPythonFunction')\n",
    "\n",
    "    def _torch_stream_wrapper(self, function, *ins):\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            out = function(*ins)\n",
    "        self.stream.synchronize()\n",
    "        return out\n",
    "\n",
    "    def torch_wrapper(self, batch_processing, function, device, *args):\n",
    "        func = function if device == 'cpu' else \\\n",
    "               lambda *ins: self._torch_stream_wrapper(function, *ins)\n",
    "        if batch_processing:\n",
    "            return ops.PythonFunction.function_wrapper_batch(func,\n",
    "                                                             self.num_outputs,\n",
    "                                                             torch.utils.dlpack.from_dlpack,\n",
    "                                                             torch.utils.dlpack.to_dlpack,\n",
    "                                                             *args)\n",
    "        else:\n",
    "            return ops.PythonFunction.function_wrapper_per_sample(func,\n",
    "                                                                  self.num_outputs,\n",
    "                                                                  torch_dlpack.from_dlpack,\n",
    "                                                                  torch_dlpack.to_dlpack,\n",
    "                                                                  *args)\n",
    "\n",
    "    def __call__(self, *inputs, **kwargs):\n",
    "        pipeline = Pipeline.current()\n",
    "        if pipeline is None:\n",
    "            Pipeline._raise_no_current_pipeline(\"TorchPythonFunction\")\n",
    "        if self.stream is None:\n",
    "            self.stream = torch.cuda.Stream(device=pipeline.device_id)\n",
    "        return super(TorchPythonFunction, self).__call__(*inputs, **kwargs)\n",
    "\n",
    "    def __init__(self, function, num_outputs=1, device='cpu', batch_processing=False, **kwargs):\n",
    "        self.stream = None\n",
    "        super(TorchPythonFunction, self).__init__(impl_name=\"DLTensorPythonFunctionImpl\",\n",
    "                                                  function=lambda *ins:\n",
    "                                                  self.torch_wrapper(batch_processing,\n",
    "                                                                     function, device,\n",
    "                                                                     *ins),\n",
    "                                                  num_outputs=num_outputs, device=device,\n",
    "                                                  batch_processing=batch_processing, **kwargs)\n",
    "\n",
    "\n",
    "ops._wrap_op(TorchPythonFunction, \"fn\", __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62affc2b",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:02.592020Z",
     "iopub.status.busy": "2022-12-14T10:09:02.591740Z",
     "iopub.status.idle": "2022-12-14T10:09:03.905587Z",
     "shell.execute_reply": "2022-12-14T10:09:03.904593Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 1.325285,
     "end_time": "2022-12-14T10:09:03.908108",
     "exception": false,
     "start_time": "2022-12-14T10:09:02.582823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "import gdcm\n",
    "import json\n",
    "import shutil\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e2fac",
   "metadata": {
    "papermill": {
     "duration": 0.007626,
     "end_time": "2022-12-14T10:09:03.923874",
     "exception": false,
     "start_time": "2022-12-14T10:09:03.916248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "- I use the same strategy as in https://www.kaggle.com/code/theoviel/dicom-resized-png-jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a0f85f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:03.941580Z",
     "iopub.status.busy": "2022-12-14T10:09:03.940571Z",
     "iopub.status.idle": "2022-12-14T10:09:03.954118Z",
     "shell.execute_reply": "2022-12-14T10:09:03.952524Z"
    },
    "papermill": {
     "duration": 0.024975,
     "end_time": "2022-12-14T10:09:03.956717",
     "exception": false,
     "start_time": "2022-12-14T10:09:03.931742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images : 4\n"
     ]
    }
   ],
   "source": [
    "IMG_PATH = \"/kaggle/input/rsna-breast-cancer-detection/test_images/\"\n",
    "test_images = glob.glob(f\"{IMG_PATH}*/*.dcm\")\n",
    "\n",
    "if DEBUG:\n",
    "    IMG_PATH = \"/kaggle/input/rsna-breast-cancer-detection/train_images/\"\n",
    "#     test_images = glob.glob(f\"{IMG_PATH}*/*.dcm\")[:1000]\n",
    "    test_images = glob.glob(f\"{IMG_PATH}10042/*.dcm\")\n",
    "    \n",
    "print(\"Number of images :\", len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff350592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:03.974803Z",
     "iopub.status.busy": "2022-12-14T10:09:03.973945Z",
     "iopub.status.idle": "2022-12-14T10:09:03.980539Z",
     "shell.execute_reply": "2022-12-14T10:09:03.979599Z"
    },
    "papermill": {
     "duration": 0.017364,
     "end_time": "2022-12-14T10:09:03.982521",
     "exception": false,
     "start_time": "2022-12-14T10:09:03.965157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAVE_FOLDER = \"/tmp/output/\"\n",
    "SIZE = 1024\n",
    "\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "\n",
    "if len(test_images) > 100:\n",
    "    N_CHUNKS = 4\n",
    "else:\n",
    "    N_CHUNKS = 1\n",
    "\n",
    "CHUNKS = [(len(test_images) / N_CHUNKS * k, len(test_images) / N_CHUNKS * (k + 1)) for k in range(N_CHUNKS)]\n",
    "CHUNKS = np.array(CHUNKS).astype(int)\n",
    "    \n",
    "J2K_FOLDER = \"/tmp/j2k/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f024e",
   "metadata": {
    "papermill": {
     "duration": 0.007455,
     "end_time": "2022-12-14T10:09:03.997799",
     "exception": false,
     "start_time": "2022-12-14T10:09:03.990344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Process jpeg compressed dicoms on GPU\n",
    "- Convert files to j2k\n",
    "- Load j2k files, resize & scale on GPU !\n",
    "- Processing is done per batch not to run out of disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab5aaed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:04.015161Z",
     "iopub.status.busy": "2022-12-14T10:09:04.014406Z",
     "iopub.status.idle": "2022-12-14T10:09:05.794563Z",
     "shell.execute_reply": "2022-12-14T10:09:05.793599Z"
    },
    "papermill": {
     "duration": 1.791477,
     "end_time": "2022-12-14T10:09:05.797086",
     "exception": false,
     "start_time": "2022-12-14T10:09:04.005609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "from nvidia.dali import pipeline_def\n",
    "from nvidia.dali.types import DALIDataType\n",
    "from pydicom.filebase import DicomBytesIO\n",
    "from nvidia.dali.plugin.pytorch import feed_ndarray, to_torch_type\n",
    "\n",
    "\n",
    "def convert_dicom_to_j2k(file, save_folder=\"\"):\n",
    "    patient = file.split('/')[-2]\n",
    "    image = file.split('/')[-1][:-4]\n",
    "    dcmfile = pydicom.dcmread(file)\n",
    "\n",
    "    if dcmfile.file_meta.TransferSyntaxUID == '1.2.840.10008.1.2.4.90':\n",
    "        with open(file, 'rb') as fp:\n",
    "            raw = DicomBytesIO(fp.read())\n",
    "            ds = pydicom.dcmread(raw)\n",
    "        offset = ds.PixelData.find(b\"\\x00\\x00\\x00\\x0C\")  #<---- the jpeg2000 header info we're looking for\n",
    "        hackedbitstream = bytearray()\n",
    "        hackedbitstream.extend(ds.PixelData[offset:])\n",
    "        with open(save_folder + f\"{patient}_{image}.jp2\", \"wb\") as binary_file:\n",
    "            binary_file.write(hackedbitstream)\n",
    "\n",
    "            \n",
    "@pipeline_def\n",
    "def j2k_decode_pipeline(j2kfiles):\n",
    "    jpegs, _ = fn.readers.file(files=j2kfiles)\n",
    "    images = fn.experimental.decoders.image(jpegs, device='mixed', output_type=types.ANY_DATA, dtype=DALIDataType.UINT16)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e95f3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:05.814998Z",
     "iopub.status.busy": "2022-12-14T10:09:05.814444Z",
     "iopub.status.idle": "2022-12-14T10:09:11.424657Z",
     "shell.execute_reply": "2022-12-14T10:09:11.423515Z"
    },
    "papermill": {
     "duration": 5.622004,
     "end_time": "2022-12-14T10:09:11.427230",
     "exception": false,
     "start_time": "2022-12-14T10:09:05.805226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220c2dc873fd4b9692c2f2abf06f2911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for chunk in tqdm(CHUNKS):\n",
    "    os.makedirs(J2K_FOLDER, exist_ok=True)\n",
    "\n",
    "    _ = Parallel(n_jobs=2)(\n",
    "        delayed(convert_dicom_to_j2k)(img, save_folder=J2K_FOLDER)\n",
    "        for img in test_images[chunk[0]: chunk[1]]\n",
    "    )\n",
    "    \n",
    "    j2kfiles = glob.glob(J2K_FOLDER + \"*.jp2\")\n",
    "\n",
    "    if not len(j2kfiles):\n",
    "        continue\n",
    "\n",
    "    pipe = j2k_decode_pipeline(j2kfiles, batch_size=1, num_threads=2, device_id=0, debug=True)\n",
    "    pipe.build()\n",
    "\n",
    "    for i, f in enumerate(j2kfiles):\n",
    "        patient, image = f.split('/')[-1][:-4].split('_')\n",
    "        dicom = pydicom.dcmread(IMG_PATH + f\"{patient}/{image}.dcm\")\n",
    "\n",
    "        out = pipe.run()\n",
    "\n",
    "        # Dali -> Torch\n",
    "        img = out[0][0]\n",
    "        img_torch = torch.empty(img.shape(), dtype=torch.int16, device=\"cuda\")\n",
    "        feed_ndarray(img, img_torch, cuda_stream=torch.cuda.current_stream(device=0))\n",
    "        img = img_torch.float()\n",
    "\n",
    "        # Scale, resize, invert on GPU !\n",
    "        min_, max_ = img.min(), img.max()\n",
    "        img = (img - min_) / (max_ - min_)\n",
    "\n",
    "        if SIZE:\n",
    "            img = F.interpolate(img.view(1, 1, img.size(0), img.size(1)), (SIZE, SIZE), mode=\"bilinear\")[0, 0]\n",
    "\n",
    "        if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "            img = 1 - img\n",
    "\n",
    "        # Back to CPU + SAVE\n",
    "        img = (img * 255).cpu().numpy().astype(np.uint8)\n",
    "\n",
    "        cv2.imwrite(SAVE_FOLDER + f\"{patient}_{image}.png\", img)\n",
    "\n",
    "    shutil.rmtree(J2K_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0853d3d8",
   "metadata": {
    "papermill": {
     "duration": 0.008015,
     "end_time": "2022-12-14T10:09:11.443853",
     "exception": false,
     "start_time": "2022-12-14T10:09:11.435838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Process the rest on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6860313",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:11.462322Z",
     "iopub.status.busy": "2022-12-14T10:09:11.461972Z",
     "iopub.status.idle": "2022-12-14T10:09:11.474577Z",
     "shell.execute_reply": "2022-12-14T10:09:11.473683Z"
    },
    "papermill": {
     "duration": 0.024035,
     "end_time": "2022-12-14T10:09:11.476463",
     "exception": false,
     "start_time": "2022-12-14T10:09:11.452428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dicomsdl\n",
    "\n",
    "def dicomsdl_to_numpy_image(dicom, index=0):\n",
    "    info = dicom.getPixelDataInfo()\n",
    "    dtype = info['dtype']\n",
    "    if info['SamplesPerPixel'] != 1:\n",
    "        raise RuntimeError('SamplesPerPixel != 1')\n",
    "    else:\n",
    "        shape = [info['Rows'], info['Cols']]\n",
    "    outarr = np.empty(shape, dtype=dtype)\n",
    "    dicom.copyFrameData(index, outarr)\n",
    "    return outarr\n",
    "\n",
    "def load_img_dicomsdl(f):\n",
    "    return dicomsdl_to_numpy_image(dicomsdl.open(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4db8877a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:11.494061Z",
     "iopub.status.busy": "2022-12-14T10:09:11.493406Z",
     "iopub.status.idle": "2022-12-14T10:09:11.500971Z",
     "shell.execute_reply": "2022-12-14T10:09:11.499829Z"
    },
    "papermill": {
     "duration": 0.018522,
     "end_time": "2022-12-14T10:09:11.503235",
     "exception": false,
     "start_time": "2022-12-14T10:09:11.484713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process(f, size=512, save_folder=\"\"):\n",
    "    patient = f.split('/')[-2]\n",
    "    image = f.split('/')[-1][:-4]\n",
    "\n",
    "    dicom = pydicom.dcmread(f)\n",
    "\n",
    "    if dicom.file_meta.TransferSyntaxUID == '1.2.840.10008.1.2.4.90':  # ALREADY PROCESSED\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        img = load_img_dicomsdl(f)\n",
    "    except:\n",
    "        img = dicom.pixel_array\n",
    "\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "    if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "        img = 1 - img\n",
    "\n",
    "    img = cv2.resize(img, (size, size))\n",
    "\n",
    "    cv2.imwrite(save_folder + f\"{patient}_{image}.png\", (img * 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba2370de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:11.520267Z",
     "iopub.status.busy": "2022-12-14T10:09:11.519977Z",
     "iopub.status.idle": "2022-12-14T10:09:11.625432Z",
     "shell.execute_reply": "2022-12-14T10:09:11.624551Z"
    },
    "papermill": {
     "duration": 0.116222,
     "end_time": "2022-12-14T10:09:11.627685",
     "exception": false,
     "start_time": "2022-12-14T10:09:11.511463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99c2a3a69994c31b3b7f10fe0abc90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = Parallel(n_jobs=2)(\n",
    "    delayed(process)(img, size=SIZE, save_folder=SAVE_FOLDER)\n",
    "    for img in tqdm(test_images)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2666b39",
   "metadata": {
    "papermill": {
     "duration": 0.007777,
     "end_time": "2022-12-14T10:09:11.643819",
     "exception": false,
     "start_time": "2022-12-14T10:09:11.636042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e035e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:11.661101Z",
     "iopub.status.busy": "2022-12-14T10:09:11.660755Z",
     "iopub.status.idle": "2022-12-14T10:09:11.667733Z",
     "shell.execute_reply": "2022-12-14T10:09:11.666549Z"
    },
    "papermill": {
     "duration": 0.017823,
     "end_time": "2022-12-14T10:09:11.669793",
     "exception": false,
     "start_time": "2022-12-14T10:09:11.651970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Parameters used for training\n",
    "    \"\"\"\n",
    "    # General\n",
    "    seed = 42\n",
    "    verbose = 1\n",
    "    device = \"cuda\"\n",
    "    save_weights = True\n",
    "\n",
    "    # Images\n",
    "    size = 1024\n",
    "\n",
    "    # k-fold\n",
    "    k = 4  # Stratified GKF\n",
    "\n",
    "    # Model\n",
    "    name = \"tf_efficientnetv2_s\"\n",
    "    pretrained_weights = None\n",
    "    num_classes = 1\n",
    "    n_channels = 3\n",
    "\n",
    "    # Training    \n",
    "    loss_config = {\n",
    "        \"name\": \"bce\",\n",
    "        \"smoothing\": 0.,\n",
    "        \"activation\": \"sigmoid\",\n",
    "    }\n",
    "\n",
    "    data_config = {\n",
    "        \"batch_size\": 8,\n",
    "        \"val_bs\": 8,\n",
    "    }\n",
    "\n",
    "    optimizer_config = {\n",
    "        \"name\": \"AdamW\",\n",
    "        \"lr\": 3e-4,\n",
    "        \"warmup_prop\": 0.1,\n",
    "        \"betas\": (0.9, 0.999),\n",
    "        \"max_grad_norm\": 10.,\n",
    "    }\n",
    "\n",
    "    epochs = 4\n",
    "    use_fp16 = True\n",
    "    \n",
    "    ## Other stuff\n",
    "    # Augmentations : Only HorizontalFlip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7e7c1",
   "metadata": {
    "papermill": {
     "duration": 0.00782,
     "end_time": "2022-12-14T10:09:11.685518",
     "exception": false,
     "start_time": "2022-12-14T10:09:11.677698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea247b7",
   "metadata": {
    "papermill": {
     "duration": 0.007678,
     "end_time": "2022-12-14T10:09:11.701210",
     "exception": false,
     "start_time": "2022-12-14T10:09:11.693532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af5092a9",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:11.719128Z",
     "iopub.status.busy": "2022-12-14T10:09:11.718341Z",
     "iopub.status.idle": "2022-12-14T10:09:11.726268Z",
     "shell.execute_reply": "2022-12-14T10:09:11.725420Z"
    },
    "papermill": {
     "duration": 0.019162,
     "end_time": "2022-12-14T10:09:11.728383",
     "exception": false,
     "start_time": "2022-12-14T10:09:11.709221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BreastDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Image torch Dataset.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        transforms=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        Args:\n",
    "            paths (list): Path to images.\n",
    "            transforms (albumentation transforms, optional): Transforms to apply. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.paths = df['path'].values\n",
    "        self.transforms = transforms\n",
    "        self.targets = df['cancer'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Item accessor\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index.\n",
    "\n",
    "        Returns:\n",
    "            np array [H x W x C]: Image.\n",
    "            torch tensor [1]: Label.\n",
    "            torch tensor [1]: Sample weight.\n",
    "        \"\"\"\n",
    "        image = cv2.imread(self.paths[idx])\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image=image)[\"image\"]\n",
    "\n",
    "        y = torch.tensor([self.targets[idx]], dtype=torch.float)\n",
    "        w = torch.tensor([1])\n",
    "\n",
    "        return image, y, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68355d9a",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:11.745847Z",
     "iopub.status.busy": "2022-12-14T10:09:11.745141Z",
     "iopub.status.idle": "2022-12-14T10:09:12.915559Z",
     "shell.execute_reply": "2022-12-14T10:09:12.914560Z"
    },
    "papermill": {
     "duration": 1.181652,
     "end_time": "2022-12-14T10:09:12.917996",
     "exception": false,
     "start_time": "2022-12-14T10:09:11.736344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_transfos(augment=True, visualize=False):\n",
    "    \"\"\"\n",
    "    Returns transformations.\n",
    "\n",
    "    Args:\n",
    "        augment (bool, optional): Whether to apply augmentations. Defaults to True.\n",
    "        visualize (bool, optional): Whether to use transforms for visualization. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        albumentation transforms: transforms.\n",
    "    \"\"\"\n",
    "    return albu.Compose(\n",
    "        [\n",
    "            albu.Normalize(mean=0, std=1),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        p=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd3fe6a5",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:12.936210Z",
     "iopub.status.busy": "2022-12-14T10:09:12.935419Z",
     "iopub.status.idle": "2022-12-14T10:09:12.943954Z",
     "shell.execute_reply": "2022-12-14T10:09:12.943124Z"
    },
    "papermill": {
     "duration": 0.019727,
     "end_time": "2022-12-14T10:09:12.945984",
     "exception": false,
     "start_time": "2022-12-14T10:09:12.926257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_model_weights(model, filename, verbose=1, cp_folder=\"\", strict=True):\n",
    "    \"\"\"\n",
    "    Loads the weights of a PyTorch model. The exception handles cpu/gpu incompatibilities.\n",
    "\n",
    "    Args:\n",
    "        model (torch model): Model to load the weights to.\n",
    "        filename (str): Name of the checkpoint.\n",
    "        verbose (int, optional): Whether to display infos. Defaults to 1.\n",
    "        cp_folder (str, optional): Folder to load from. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        torch model: Model with loaded weights.\n",
    "    \"\"\"\n",
    "    state_dict = torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\")\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(state_dict, strict=strict)\n",
    "    except BaseException:\n",
    "        try:\n",
    "            del state_dict['logits.weight'], state_dict['logits.bias']\n",
    "            model.load_state_dict(state_dict, strict=strict)\n",
    "        except BaseException:\n",
    "            del state_dict['encoder.conv_stem.weight']\n",
    "            model.load_state_dict(state_dict, strict=strict)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n -> Loading encoder weights from {os.path.join(cp_folder,filename)}\\n\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07fd3149",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:12.963949Z",
     "iopub.status.busy": "2022-12-14T10:09:12.963035Z",
     "iopub.status.idle": "2022-12-14T10:09:12.971485Z",
     "shell.execute_reply": "2022-12-14T10:09:12.970535Z"
    },
    "papermill": {
     "duration": 0.019451,
     "end_time": "2022-12-14T10:09:12.973477",
     "exception": false,
     "start_time": "2022-12-14T10:09:12.954026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "\n",
    "def predict(model, dataset, loss_config, batch_size=64, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Torch predict function.\n",
    "\n",
    "    Args:\n",
    "        model (torch model): Model to predict with.\n",
    "        dataset (CustomDataset): Dataset to predict on.\n",
    "        loss_config (dict): Loss config, used for activation functions.\n",
    "        batch_size (int, optional): Batch size. Defaults to 64.\n",
    "        device (str, optional): Device for torch. Defaults to \"cuda\".\n",
    "\n",
    "    Returns:\n",
    "        numpy array [len(dataset) x num_classes]: Predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = np.empty((0,  model.num_classes))\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[0].to(device)\n",
    "\n",
    "            # Forward\n",
    "            pred, pred_aux = model(x)\n",
    "\n",
    "            # Get probabilities\n",
    "            if loss_config['activation'] == \"sigmoid\":\n",
    "                pred = pred.sigmoid()\n",
    "            elif loss_config['activation'] == \"softmax\":\n",
    "                pred = pred.softmax(-1)\n",
    "\n",
    "            preds = np.concatenate([preds, pred.cpu().numpy()])\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d358a173",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:12.991047Z",
     "iopub.status.busy": "2022-12-14T10:09:12.990757Z",
     "iopub.status.idle": "2022-12-14T10:09:15.066801Z",
     "shell.execute_reply": "2022-12-14T10:09:15.065427Z"
    },
    "papermill": {
     "duration": 2.087862,
     "end_time": "2022-12-14T10:09:15.069356",
     "exception": false,
     "start_time": "2022-12-14T10:09:12.981494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/timm-0-6-9/pytorch-image-models-master')\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def define_model(\n",
    "    name,\n",
    "    num_classes=1,\n",
    "    num_classes_aux=0,\n",
    "    n_channels=1,\n",
    "    pretrained_weights=\"\",\n",
    "    pretrained=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a pretrained model & builds the architecture.\n",
    "    Supports timm models.\n",
    "\n",
    "    Args:\n",
    "        name (str): Model name\n",
    "        num_classes (int, optional): Number of classes. Defaults to 1.\n",
    "        num_classes_aux (int, optional): Number of aux classes. Defaults to 0.\n",
    "        n_channels (int, optional): Number of image channels. Defaults to 3.\n",
    "        pretrained_weights (str, optional): Path to pretrained encoder weights. Defaults to ''.\n",
    "        pretrained (bool, optional): Whether to load timm pretrained weights.\n",
    "\n",
    "    Returns:\n",
    "        torch model -- Pretrained model.\n",
    "    \"\"\"\n",
    "    # Load pretrained model\n",
    "    encoder = getattr(timm.models, name)(pretrained=pretrained)\n",
    "    encoder.name = name\n",
    "\n",
    "    # Tile Model\n",
    "    model = ClsModel(\n",
    "        encoder,\n",
    "        num_classes=num_classes,\n",
    "        num_classes_aux=num_classes_aux,\n",
    "        n_channels=n_channels,\n",
    "    )\n",
    "\n",
    "    if pretrained_weights:\n",
    "        model = load_model_weights(model, pretrained_weights, verbose=1, strict=False)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class ClsModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model with an attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        num_classes=1,\n",
    "        num_classes_aux=0,\n",
    "        n_channels=3,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Args:\n",
    "            encoder (timm model): Encoder.\n",
    "            num_classes (int, optional): Number of classes. Defaults to 1.\n",
    "            num_classes_aux (int, optional): Number of aux classes. Defaults to 0.\n",
    "            n_channels (int, optional): Number of image channels. Defaults to 3.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.nb_ft = encoder.num_features\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_classes_aux = num_classes_aux\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "        self.logits = nn.Linear(self.nb_ft, num_classes)\n",
    "        if self.num_classes_aux:\n",
    "            self.logits_aux = nn.Linear(self.nb_ft, num_classes_aux)\n",
    "\n",
    "        self._update_num_channels()\n",
    "\n",
    "    def _update_num_channels(self):\n",
    "        if self.n_channels != 3:\n",
    "            for n, m in self.encoder.named_modules():\n",
    "                if n:\n",
    "                    # print(\"Replacing\", n)\n",
    "                    old_conv = getattr(self.encoder, n)\n",
    "                    new_conv = nn.Conv2d(\n",
    "                        self.n_channels,\n",
    "                        old_conv.out_channels,\n",
    "                        kernel_size=old_conv.kernel_size,\n",
    "                        stride=old_conv.stride,\n",
    "                        padding=old_conv.padding,\n",
    "                        bias=old_conv.bias is not None,\n",
    "                    )\n",
    "                    setattr(self.encoder, n, new_conv)\n",
    "                    break\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"\n",
    "        Extract features function.\n",
    "\n",
    "        Args:\n",
    "            x (torch tensor [batch_size x 3 x w x h]): Input batch.\n",
    "\n",
    "        Returns:\n",
    "            torch tensor [batch_size x num_features]: Features.\n",
    "        \"\"\"\n",
    "        fts = self.encoder.forward_features(x)\n",
    "\n",
    "        while len(fts.size()) > 2:\n",
    "            fts = fts.mean(-1)\n",
    "\n",
    "        return fts\n",
    "\n",
    "    def get_logits(self, fts):\n",
    "        \"\"\"\n",
    "        Computes logits.\n",
    "\n",
    "        Args:\n",
    "            fts (torch tensor [batch_size x num_features]): Features.\n",
    "\n",
    "        Returns:\n",
    "            torch tensor [batch_size x num_classes]: logits.\n",
    "            torch tensor [batch_size x num_classes_aux]: logits aux.\n",
    "        \"\"\"\n",
    "        logits = self.logits(fts)\n",
    "\n",
    "        if self.num_classes_aux:\n",
    "            logits_aux = self.logits_aux(fts)\n",
    "        else:\n",
    "            logits_aux = torch.zeros((fts.size(0)))\n",
    "\n",
    "        return logits, logits_aux\n",
    "\n",
    "    def forward(self, x, return_fts=False):\n",
    "        \"\"\"\n",
    "        Forward function.\n",
    "\n",
    "        Args:\n",
    "            x (torch tensor [batch_size x n_frames x h x w]): Input batch.\n",
    "\n",
    "        Returns:\n",
    "            torch tensor [batch_size x num_classes]: logits.\n",
    "            torch tensor [batch_size x num_classes_aux]: logits aux.\n",
    "        \"\"\"\n",
    "        fts = self.extract_features(x)\n",
    "\n",
    "        logits, logits_aux = self.get_logits(fts)\n",
    "\n",
    "        return logits, logits_aux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c274b6",
   "metadata": {
    "papermill": {
     "duration": 0.007879,
     "end_time": "2022-12-14T10:09:15.085822",
     "exception": false,
     "start_time": "2022-12-14T10:09:15.077943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bef9b4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:15.104074Z",
     "iopub.status.busy": "2022-12-14T10:09:15.103147Z",
     "iopub.status.idle": "2022-12-14T10:09:15.128437Z",
     "shell.execute_reply": "2022-12-14T10:09:15.127580Z"
    },
    "papermill": {
     "duration": 0.036542,
     "end_time": "2022-12-14T10:09:15.130582",
     "exception": false,
     "start_time": "2022-12-14T10:09:15.094040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/rsna-breast-cancer-detection/test.csv\")\n",
    "df['cancer'] = 0\n",
    "\n",
    "if DEBUG:\n",
    "    df = pd.read_csv(\"/kaggle/input/rsna-breast-cancer-detection/train.csv\")\n",
    "    df['path'] = SAVE_FOLDER + df[\"patient_id\"].astype(str) + \"_\" + df[\"image_id\"].astype(str) + \".png\"\n",
    "    df = df[df['path'].apply(lambda x: os.path.exists(x))].reset_index(drop=True)\n",
    "\n",
    "df['path'] = SAVE_FOLDER + df[\"patient_id\"].astype(str) + \"_\" + df[\"image_id\"].astype(str) + \".png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fd3188c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:15.148355Z",
     "iopub.status.busy": "2022-12-14T10:09:15.147555Z",
     "iopub.status.idle": "2022-12-14T10:09:15.152474Z",
     "shell.execute_reply": "2022-12-14T10:09:15.151667Z"
    },
    "papermill": {
     "duration": 0.016104,
     "end_time": "2022-12-14T10:09:15.154552",
     "exception": false,
     "start_time": "2022-12-14T10:09:15.138448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_TTA = False\n",
    "predict_fct = predict_tta if USE_TTA else predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7f37db9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:15.171766Z",
     "iopub.status.busy": "2022-12-14T10:09:15.171492Z",
     "iopub.status.idle": "2022-12-14T10:09:15.175241Z",
     "shell.execute_reply": "2022-12-14T10:09:15.174362Z"
    },
    "papermill": {
     "duration": 0.014734,
     "end_time": "2022-12-14T10:09:15.177320",
     "exception": false,
     "start_time": "2022-12-14T10:09:15.162586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [\n",
    "    \"/kaggle/input/rsna-breast-weights-public/tf_efficientnetv2_s_1024/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4bd47",
   "metadata": {
    "papermill": {
     "duration": 0.00783,
     "end_time": "2022-12-14T10:09:15.193074",
     "exception": false,
     "start_time": "2022-12-14T10:09:15.185244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "775eb2d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:15.209919Z",
     "iopub.status.busy": "2022-12-14T10:09:15.209642Z",
     "iopub.status.idle": "2022-12-14T10:09:15.722247Z",
     "shell.execute_reply": "2022-12-14T10:09:15.720921Z"
    },
    "papermill": {
     "duration": 0.524044,
     "end_time": "2022-12-14T10:09:15.724942",
     "exception": false,
     "start_time": "2022-12-14T10:09:15.200898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No weights found, add your own dataset !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "for exp_folder in EXP_FOLDERS:\n",
    "    config = Config\n",
    "\n",
    "    model = define_model(\n",
    "        config.name,\n",
    "        num_classes=config.num_classes,\n",
    "        num_classes_aux=0,\n",
    "        n_channels=3,\n",
    "        pretrained=False\n",
    "    )\n",
    "    model = model.cuda().eval()\n",
    "\n",
    "    dataset = BreastDataset(\n",
    "        df,\n",
    "        transforms=get_transfos(augment=False),\n",
    "    )\n",
    "    \n",
    "    weights = sorted(glob.glob(exp_folder + f\"*.pt\"))\n",
    "    if not len(weights):\n",
    "        print('No weights found, add your own dataset !')\n",
    "\n",
    "    preds = []\n",
    "    for fold, weight in enumerate(weights):\n",
    "        model = load_model_weights(model, weight, verbose=1)\n",
    "        pred = predict(model, dataset, config.loss_config, batch_size=8)\n",
    "        preds.append(pred)\n",
    "\n",
    "    preds = np.mean(preds, 0)\n",
    "    all_preds.append(preds)\n",
    "    \n",
    "preds_blend = np.mean(all_preds, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c68fd1",
   "metadata": {
    "papermill": {
     "duration": 0.007931,
     "end_time": "2022-12-14T10:09:15.741182",
     "exception": false,
     "start_time": "2022-12-14T10:09:15.733251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Blend, PP & Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "302cc07e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T10:09:15.760793Z",
     "iopub.status.busy": "2022-12-14T10:09:15.759058Z",
     "iopub.status.idle": "2022-12-14T10:09:15.788152Z",
     "shell.execute_reply": "2022-12-14T10:09:15.787064Z"
    },
    "papermill": {
     "duration": 0.040993,
     "end_time": "2022-12-14T10:09:15.790544",
     "exception": false,
     "start_time": "2022-12-14T10:09:15.749551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction_id</th>\n",
       "      <th>cancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10008_L</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10008_R</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prediction_id  cancer\n",
       "0       10008_L       0\n",
       "1       10008_R       0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "THRESHOLD = 0.2\n",
    "\n",
    "df[\"cancer\"] = preds_blend\n",
    "df['prediction_id'] = df['patient_id'].astype(str) + \"_\" + df['laterality']\n",
    "\n",
    "sub = df[['prediction_id', 'cancer']].groupby(\"prediction_id\").mean().reset_index()\n",
    "sub[\"cancer\"] = (sub[\"cancer\"] > THRESHOLD).astype(int)\n",
    "\n",
    "sub.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e1eec",
   "metadata": {
    "papermill": {
     "duration": 0.007921,
     "end_time": "2022-12-14T10:09:15.806530",
     "exception": false,
     "start_time": "2022-12-14T10:09:15.798609",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Done !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 128.213239,
   "end_time": "2022-12-14T10:09:18.534701",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-14T10:07:10.321462",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03d9ad5105aa4a6f86f380931f0f979a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1022b47a64f04ffd857aee19a014d3f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3577f33844384d5ebc157a2e68866e62",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ed137f2891e9437fa21d0f7160565d4a",
       "value": 1
      }
     },
     "220c2dc873fd4b9692c2f2abf06f2911": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4e808bd95c974261bf114a4b497523ed",
        "IPY_MODEL_1022b47a64f04ffd857aee19a014d3f2",
        "IPY_MODEL_852e07ad9eb14902b1e19de98ecb1a04"
       ],
       "layout": "IPY_MODEL_252104e8aff84412a3178ba9b31c1491"
      }
     },
     "252104e8aff84412a3178ba9b31c1491": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "32be856eb85d481880acb54dc8b1af0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3577f33844384d5ebc157a2e68866e62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "388e8855c3cd4236ba3841f9e56b4077": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "482102c2a6c5422caae10ed9a06b5259": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4e808bd95c974261bf114a4b497523ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bb59e7194d3743ac8feb5fde0a59ec28",
       "placeholder": "",
       "style": "IPY_MODEL_f5a281fc04e24dcc9ed225bdcbd5ec31",
       "value": "100%"
      }
     },
     "82cbb98d533449c3a6a7009e22ee4b8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f875d321519949f680b01317128ed1b3",
       "placeholder": "",
       "style": "IPY_MODEL_f4d00824398a4484980f8b1aae27c71f",
       "value": "100%"
      }
     },
     "852e07ad9eb14902b1e19de98ecb1a04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a7be2f0dca1d4e6fa020ac9b8cae3547",
       "placeholder": "",
       "style": "IPY_MODEL_d423f88be5614484b28273704a451dd3",
       "value": " 1/1 [00:05&lt;00:00,  5.59s/it]"
      }
     },
     "8ff97232c4074c98bfe521b871f72ed1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_482102c2a6c5422caae10ed9a06b5259",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_32be856eb85d481880acb54dc8b1af0d",
       "value": 4
      }
     },
     "a7be2f0dca1d4e6fa020ac9b8cae3547": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bb59e7194d3743ac8feb5fde0a59ec28": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d423f88be5614484b28273704a451dd3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ed137f2891e9437fa21d0f7160565d4a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f4072776ea374c4eac92fcc31d2c3df8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_03d9ad5105aa4a6f86f380931f0f979a",
       "placeholder": "",
       "style": "IPY_MODEL_388e8855c3cd4236ba3841f9e56b4077",
       "value": " 4/4 [00:00&lt;00:00, 45.53it/s]"
      }
     },
     "f4d00824398a4484980f8b1aae27c71f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f5a281fc04e24dcc9ed225bdcbd5ec31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f875d321519949f680b01317128ed1b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f99c2a3a69994c31b3b7f10fe0abc90d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_82cbb98d533449c3a6a7009e22ee4b8c",
        "IPY_MODEL_8ff97232c4074c98bfe521b871f72ed1",
        "IPY_MODEL_f4072776ea374c4eac92fcc31d2c3df8"
       ],
       "layout": "IPY_MODEL_faab1d2c855c46b0a4ccf46c7144d676"
      }
     },
     "faab1d2c855c46b0a4ccf46c7144d676": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
